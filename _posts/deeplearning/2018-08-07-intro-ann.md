---
title: Introduction to Artificial Neural Networks
categories: dl
layout: post
mathjax: true
typora-root-url: ../../
---

**DRAFT**

----

{% include toc.html %}

# Implementing DNN with Tensorflow

The following section implements Deep Neural Network (DNN) using core tensorflow libraries. 

## Construction Phase

### Forward Propagation

> Logits $-$ The output of the neural network *before* going through the softmax activation function.

```python
with tf.name_scope("dnn"):
    hidden1 = tf.layers.dense (X      , n_hidden1, name="hidden1", activation=tf.nn.relu)
    hidden2 = tf.layers.dense (hidden1, n_hidden2, name="hidden2", activation=tf.nn.relu)
    logits  = tf.layers.dense (hidden2, n_outputs, name="output")
```
`tf.layers.dense ` takes care of 
- Creating a fully connected (every neuron to every other neuron in consecutive) layers.
- Weights and biases with appropriate initalization (Weights initalized to gausian distribution with standard deviation of $\frac{2}{\sqrt{n_{l-1}}}$  gives good results)
- Specify the activation function for each layer $-$ **Note:** The activation funciton is not specified for the last/output layer.

### Cost Function

Cost function is higher depending on how much we are deviating from the expected value `y`. Essentially, it is the average error. 

In case of neural networks, the cost function is calucated by comparing the output of the last layer `logits` with the expected output `y`

> Cross Entropy: How much the acual probability deviates from the expected.
>
> Let `y` be the expected probability vector with 1 at a neuron position indicating expected class. Let `y_pred` be the probability vector having the predicted probability for each class. The cost function should penalize incorrect prediction.
> $$
> np.sum ( -1*y* log(y\_pred))
> $$
> In matrix positions where y is zero, the product is zero $-$ Don't care about prediction for incorrect class. 
>
> In matrix position where y is one, we expect the prediction to be closer to 1. If so, the product will be closer to zero. If the predicion is closer to zero it is a huge deviation, `-1 * 1 * log(0)` is $\infin$
>
> Thus cross entroy penalizes deviation of probability for expected class. Hence can be used for cost function.

```python
xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
cost = tf.reduce_mean(xentropy, name="cost")

# Use optimizer (GradientDescent/Adam) to minimize cost
learning_rate = 0.01
with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    training_op = optimizer.minimize(cost)
```

`sparse_softmax_cross_entropy_with_logits` computes

- Cross entropy by taking `logits` and `y` as parameters. (actual and expected). 
- Takes care of calucating the the softmax for the last layer and then calculating cross entropy for each instance.

The total cost shall be the mean of entropy across instances. The mean caculation is done by `tf.reduce_mean`

### Evaluate model - Accuracy

```python
with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
```

- `tf.nn.in_top_k` returns (n x 1)  boolean array $-$ true, if the expected class matches the actual.
- Convert boolean to float and calculate mean to get accuracy

## Execution Phase

```python
# Read data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/")

n_epochs = 40
batch_size = 50
with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        for iteration in range(mnist.train.num_examples // batch_size):
            X_batch, y_batch = mnist.train.next_batch(batch_size)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        # Evaluate model on the last mini batch (Cross validation?)    
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test  = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})
        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)
	
    # Save model to disk
    save_path = saver.save(sess, "./my_model_final.ckpt")
```

## Use trained model

```python
with tf.Session() as sess:
    # Load model from disk
    saver.restore(sess, "./my_model_final.ckpt")
    X_new_scaled = [...]  # some new images (scaled from 0 to 1)
    Z = logits.eval(feed_dict={X: X_new_scaled})
    y_pred = np.argmax(Z, axis=1)
```

