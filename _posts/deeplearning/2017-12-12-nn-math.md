---
$$ntitle: Neural network math
categories: dl
layout: post
mathjax: true
---

{% include toc.html %}

# Introduction

In order to compute the equations for forward and back propagation, consider the a 4 layer neural network  as given below.

![4LayerNN]({{"/assets/images/4LayerFullNN.png" | absolute_url}}) 

## Convention

| Item      |          Dimension           | Detail                                   |
| --------- | :--------------------------: | ---------------------------------------- |
| $$l$$     |              -               | Layer index                              |
| $$L$$     |              -               | Total number of layers = Number of hidden layers + output layer |
| $$n_{l}$$ |              -               | Number of neurons in $$l^{th}$$ layer    |
| $$n_{L}$$ |                              | Number of neurons in the last layer. That is, $$l = L$$ |
| $$m$$     |              -               | Number of training examples.             |
| $$X$$     |        $$n \times m$$        | Input dataset. Each column is an example with n features. |
| $$Y$$     |      $$n_{L} \times m$$      | Expected output dataset.                 |
| $$W_{l}$$ | $$n_{l}  \ \times\ n_{l-1}$$ | The weight matrix                        |
| $$Z_{l}$$ |      $$n_{l} \times m$$      | $$l^{th}$$ layer receiving inputs from previous layer |
| $$A_{l}$$ |      $$n_{l} \times m$$      | $$l^{th}$$ activation layer. Input layer for the next layer. |
| $$A_{L}$$ |      $$n_{L} \times m$$      | Last activation layer                    |

# Forward Propagation

## Generic Equation

The generic equation for forward propagation is as follows. Here, g(z) is an activation function such as sigmoid, tanh,  relu or softmax function
$$
\begin{aligned}
Z_{l} &= W_{l}*A_{l-1} + b_{l} \\
A_{l} &= g(Z_{l})
\end{aligned}
$$

## Equations for the layers

The equation can be extended to various layers as follows:

| Layer 1            | Layer 2             | Layer 3               | Layer4              |
| ------------------ | ------------------- | --------------------- | ------------------- |
| $$Z1 = W1*X + b1$$ | $$Z2 = W2*A1 + b2$$ | $$Z3 = W3 * A2 + b3$$ | $$Z4 = W4*A3 + b4$$ |
| $$A1=g(Z1)$$       | $$A2=g(Z2)$$        | $$A3=g(Z3)$$          | $$A4=g(Z4)$$        |



# Cost Function

The cost function comparing the output of the last layer $$ A_{L}$$ and expected output layer $$Y$$ is given  as follows.

$$
J = J(w,b) =  -\frac{1}{m} \sum^{m}_{i=1} \left[ \ \sum^{n_{L}}_{j=1} \left[ \ Y \ log(A_{L}) + (1-Y) \ log(1-A_{L}) \ \right] \ \right]
$$

# Gradient descent

Gradient descent involves minimizing the cost function (*or error function*) $$J$$

- Finding the gradient $$dW = \frac{\partial J}{\partial W}$$ of the weights $$w$$ and $$db = \frac{\partial J}{\partial b}$$ of bias b w.r.t the cost function $$J(w,b)$$ so as to minimize the cost function. 
- Update the weights with the gradient

$$
\begin{aligned}
& foreach \ (W,b) \ across \ layers \ 1 \ to \ L \ \{ \\
& \ \ \ \ \ \ W := W - \alpha \ dW \\
& \ \ \ \ \ \ b := b - \alpha \ db \\
& \}
\end{aligned}
$$

>
> In order to proceed with gradient descent we need $$dW$$ and $$db$$ for each layer
>

# Back Propagation

Considering the 4 layer NN, we need to compute $$dW1, \ dW2,  \ dW3  \ and  \ dW4$$ along with $$db1, \ db2,  \ db3  \ and  \ db4$$









