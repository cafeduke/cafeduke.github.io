---
title: Object Detection
categories: dl-cnn
layout: post
mathjax: true
typora-root-url: ../../../
---



DRAFT

____



{% include toc.html %}

# Introduction

Following is a broad classification of computer vision problems

- Image classification $$-$$ Classifying an image into a class (Car, Pedestrian, Bike etc)
- Single object detection $$-$$ Classify image into a class + drawing a bounding box (co-ordinates, width and height) around the object.
- Multiple objects detection $$-$$ Detect multiple objects in an image.

Furthermore, there could many objects of the same type and/or different types in a single image. For example there could be an image with several cars, bikes and pedestrians. 

# Single object detection

The section deals with classification of an image (This is an image of what?) and locating the object (where is the object present?). This deals with a case where **only one object of one class is present**. 

## Bounding box detection

A typical image classification problem for say 4 classes (Pedestrian, Car, Bike and None) will have a softmax with 4 neurons. In essence, the `y_cap` is just a number ranging from 1-4 indicating the predicted class. 

In case of object localization `y` is a vector with following elements $$\left[ p_c, b_x, b_y, b_w, b_h, c_1, c_2, c_3 \right]$$

| Components of y   | Detail                                                       |
| ----------------- | ------------------------------------------------------------ |
| $$c_1, c_2, c_3$$ | Three different classes of objects $$-$$ Pedestrian, Car, Bike<br />Eg: [0, 1, 0] indicates that a Car is present. |
| $$p_c$$           | Probability that object belonging to any of the classes exists? <br />Eg: A zero indicates no object exists. A one indicates one of the objects exist. |
| $$b_x, b_y$$      | The x and y distances of the **bounding box** measured from left hand top corner. <br /> Eg: $$b_x$$ = 0.4 means x co-ordinate = 40% of the image size. |
| $$b_w, b_h$$      | Width and height of the **bounding box** measured from left hand top corner.<br />Eg: $$b_w$$ = 0.3 means box width = 30% of the image size. |

Note

- If $$p_c$$ = 1 then we know an object is present so, we care about which class and where?
- If $$p_c$$ = 0 then we don't care about the remaining values of `y`.

> For each training image, the expected output `y` is manually generated by **drawing a bounding box** around the image and measuring the co-ordinates $$b_x, b_y$$, measuring the width and height $$b_w, b_h$$ of the bounding box.

## Cost Function

The cost function in case of object classification + object location is

- If $$p_c == 1$$ $$-$$ Distance between the `y_cap` and `y` vectors.
- If $$p_c == 0$$ $$-$$ Distance between the first element of `y_cap` and `y` vectors. That is $$\sqrt{(y\_cap_{p_c} - y_{p_c})^2} $$

# Landmark detection

In certain applications like **facial feature recognition**, we could train a CNN to detect landmark locations on a face. For example, corner of the eye, edges of eye, corner of lip etc.

>  A landmark in general is a distinguished location that stands out or demarks otherwise seamless regions. 

![Landmark](/assets/images/dl/Landmark.png)

In the above the red dots are the landmarks. For each landmark corresponding co-ordinates ($$(l_x, l_y)$$ needs to be recorded. 

With 64 landmarks for a face 

- There are a total of $$ 64 \times 2 = 128 $$ points. 
- One classification probability $$p$$  that detects if the image is a face or not.

The $$y$$ vector will be $$\left[ p, l1_x, l1_y, l2_x, l2_y, l3_x, l3_y, ....., l64_x, l64_y \right]$$

> For each training image, the expected output `y` is manually generated by laboriously recording the landmark points on the face.

A network that identifies landmarks can be used for applications such as

- Emotion detection
- Pose detection for full body landmarks
- Facial effects like add hat, crown, cooling glass

# Detecting multiple objects 

In the previous section we have looked at classification and location of a single object. However, an image could have multiple objects. One of the ways of detecting multiple objects is using a sliding windows detection.

## Traditional sliding windows 

The sliding window method involves 

1. Train a ConvNet (CNN) that can classify an image into Car, Pedestrian, Bike or None
2. Set crop size to smallest possible
3. For each crop size
   1. Begin cropping from left hand top corner and slide to right and then down to cover the entire image
   2. Feed each crop to the ConvNet to detect objects.
4. Repeat step 3 with the next crop size
The huge **disadvantage** of sliding window is the **computational cost** of evaluating a large number of cropped images 

## Transform convolution-NN mix to pure CNN layer

Lets first look at converting a convolution to NN layer connection into a pure convolution layer.

### Convolution - NN layer

Consider an input  convolution layer (3 x 3 x 20) with 180 cells, connected to a single output NN layer with 300 neurons. 

| Input Layer Volume | Output Layer Neuron Count |
| ------------------ | ------------------------- |
| 3 x 3 x 20         | 300                       |

- Every cell of the input layer is connected to every neuron of the output layer
- This results in 180 x 300 that is 54,000 connections.

### Pure CNN layer

| Input Layer Volume | Filter Volume | Filter Count | Output Layer Volume |
| ------------------ | ------------- | ------------ | ------------------- |
| 3 x 3 x 20         | 3 x 3 x 20    | 300          | 1 x 300             |

- Every cell in the input layer volume is connected to every cell in the output layer volume
- Each filter volume gives the weights for each set of connections.

## Convolutional sliding windows 

Now that we know how to create a pure convolutional implementation. Lets see how to optimize the traditional sliding windows using pure convolution.

### Train Pure CNN - Detect single object

First, lets train a pure CNN that can classify and locate a single object.

![SlidingConvolution01](/assets/images/dl/SlidingConvolution01.png)

In this example

- The input image is of 14 x 14 resolution with 3 RGB channels. 
- The final output is (1 x 1 x 4) identifying one of 4 possible classes (Car, Pedestrian, Bike, None)
- Once the above CNN network is trained the filters will have the weights

### Use pure CNN to detect objects in a large image



![SlidingConvolution02](/assets/images/dl/SlidingConvolution02.png)



Had we used traditional sliding windows method

- We would have to crop 14 x 14 sections out of the 28 x 28 image (shown in orange) and then feed it to the Pure CNN. 
- Slide by 2 (stride=2), crop and feed to pure CNN again. 
- We would need 64 crops to cover the entire image 

Instead we now, feed the entire image to the above **trained pure CNN** network with the **same filters and weights** 

- By using the same filters we get a final output volume of dimension (8 x 8 x 4)
- The first cell corresponds to the first (14 x 14) crop, the second corresponds to the second (14 x 14) crop and the third crop is highlighted in orange.
- In essence, the output of all 64 (14 x 14) crops, obtained in one shot. 
- We can now go through each cell of the output layer, to see which class each cell has identified. 

## Limitations

- Requires training multiple networks for various crop sizes and running them which is still elaborate task
- The bounding box may not be accurate
- @Validate: Will not work for two or more objects that are close to each other and thus (of same or different class) appearing in the same crop.

# YOLO (You Look Only Once)

## Overview

Several of the limitations of convolution sliding windows is overcome by the YOLO algorithm. The YOLO algorithm does not require running the image through multiple networks, each trained for different crop size. The YOLO algorithm detect objects, just like a human $$-$$ looks only once.  The YOLO is also more accurate in locating the bounding box. 

![Yolo_overview](/assets/images/dl/Yolo_overview.png)

The overview of the algorithm is as follows:
- Divide the image into grids (9 grids, in the above case)
- Each grid shall take part in image classification and localization.
  - Each grid shall output the following vectors $$\left[ p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3 \right]$$ to indicate the presence of the object, mid-point, height and width of object and the class of the object respectively.
  - The total number of values output per image is  $$ 3 \times 3 \times 8 = 72$$.
- The network is trained to output 72 values ($$y_{pred}$$) and is compared with the 72 expected values of $$y$$ 
- The labeled data or $$y$$, per grid is determined as follows 
  - The probability that a class exists $$p_c$$, is here a boolean, set to 1 (object present) or 0 (object absent) as follows
    - If mid-point (yellow dot) of an object (Class, Bike or pedestrian) is part of the grid then $$p_c$$ is set to $$1$$
    - Even if parts of object(s) are in the grid, as long as mid-point is not inside the grid, $$p_c$$ shall be $$0$$ and rest of the values are don't cares.
  - Assuming the top left corner of the grid to be (0,0) and bottom right (1,1)
    - $$b_x$$ is the distance of the mid-point along the x-axis. Range: $$ 0 < b_x < 1 $$
    - $$b_y$$ is the distance of the mid-point along the y-axis. Range: $$ 0 < b_y < 1 $$
    - $$b_w$$ is the width of the object relative to the width of the grid, which is 1. Range: $$b_w$$ can be greater than 1.
    - $$b_h$$ is the height of the object relative to the height of the grid, which is 1. Range: $$b_h$$ can be greater than 1.
  - If mid-point of the grid has an object, set corresponding class among $$c1, c2, c3$$ to 1 

## Evaluate bounding box - IoU algorithm