---
title: Introduction to Convolutional Neural Networks
categories: dl-cnn
layout: post
mathjax: true
typora-root-url: ../../../
---

{% include toc.html %}

# Computer Vision

Computer vision is one of the areas were deep learning performs very well. Deep learning computer vission has the following applications

- Handwriting analysis. Identify plant by the image of its flower/leaf
  - Image classification
- Self driving cars
  - Object detection: Detect other cars and pedestrians
  - Object localization: Determine the location (position and size) of the objects
- Face recoginision $$-$$ Use face as ID for authentication.
- Face Styling $$-$$ Add features like hair style, eye style, nose modification etc to the face. 
- New art $$-$$ Merge images.
- Transfer learning $$-$$ Ideas learnt in computer vision can be useful in speech recognision.

# Limitation of NN for computer vision

One of the primary limitation of NN the exponential increase in memory, computation resources and computation time required with just average image resolutions by today's standards (Eg resolution: 1000x1000)

Sample neuron overhead calucation for a 1000x1000 image

- Each column in X will be 1000 x 1000 x 3 (considering RGB) = 3 million
- Let the first hidden layer to have only 1000 neurons 
- W1.shape = (1000,  3M) = 3 billion weights

Huge number of neurons greatly increases the chances of *overfitting* and increases computational overhead. However, image size cannot be compromized $$-$$ Enter CNN (*Convolutional Neural Netrowks*).

# Convolution

Below we take a look at how a basic convolution operation is performed. A single image layer (As against considering 3 channels of RGB) is convolved (operation denoted by '*') with a single filter layer to produce a resultant layer.

## Basic Convolution

The **convolve**  operation requires an **input** layer (6x6 in this case) and a **filter** layer (3x3 in this case) to produce an **ouput** layer.

![Convolution](/assets/images/dl/Convolution.png)

### Operation Details

- The filter layer is super imposed on the input layer starting with the left hand top corner
- The overlapping numbers are multiplied. The products are then added to get the value of a cell in the resultant matrix.
- The following operations are perfomed to fill cells of the resultant matrix
  - The filter is shifted right hand by one cell (stride = 1) as long as possible
  - The filter is shifted down by one cell as long as possible

### Example

Cell (1,2) of the resultant matrix as shown in the diagram above is obtained as follows:

```mathematica
Cell(1,2) = (1*0 + 0*1 + -1*2) + (1*5 + 0*8 + -1*9) + (1*7 + 0*2 + -1*5)
Cell(1,2) = (0+0-2) + (5+0-9) + (7+0-5)
Cell(1,2) = (-2) + (-4) + (2)
Cell(1,2) = -4
```

## Filter

The input/output image is analogous to a NN layer and the filter is analogous to the weight.  

Filter detects a pattern. 

- To establish this consider a gray scale image. 
- Higher value gets a whiter shade and lower value gets darker shade.
- With this convention, the above filter has stripes from light to dark. (1 is lightest and -1 is darkest)
- In essense, the above filter detects vertical edges.

### Standard filter - Vertical edge detection

In the below example the vertical edge detector filter (with transitions from light to dark, as seen above) is applied to an image. 

#### Example


![VerticalEdge](/assets/images/dl/VerticalEdge.png)



#### Obervation - Selected Cell 

- Green square 
  - It is just a light box (number 10s only) without any edges
  - The corresponding resultant matrix cell has 0 which indicates there is no edge detection.
- Red square
  - Here we have light to dark transition
  - The corresponding resultant matrix cell has 30 which indicates an edge detection. 
  - A positive number indicates that edge transition is from light to dark (accordance with filter).

#### Obervation - Overall

- The input image has light and dark colums
- After convolution we see that the resultant layer identifies a thick white edge (number 30s) right in the middle.
- The resultant convolution is referrering to the vertical edge seperating the light layer (number 10s) from the dark layer (number 0s)

#### Conclusion

The output image indicates that the input image has a vertical edge in the center. The filter acts as vertical edge detector.

### Standard filter - Horizontal edge detection

In the image is like a chess board with both vertical and horizontal images.

#### Example

![ChessBox](/assets/images/dl/ChessBox.png)



#### Obervation - Selected Cell 

- Blue Square	$-$ No edge. Resultant value = 0.
- Green Square $$-$$ Horizontal edge found. Resultant value = 30.
- Yellow Sqaure $$-$$ Two horizontal edge. The positive one (Goes from 10 to 0. In accordance with filter) is larger than the negative one (Goes from 0 to 10). Resultant value = 10
- Purple square $$-$$ Horizontal edge found, but negative. Resultant value = -30.

### Calculation

Let `nxn` be the size of input matrix and `fxf` be the size of the filter.
$$
Output \ Layer \ Size = \ (n-f+1, n-f+1)
$$


## Padding

Padding is a process of making the input matrix larger by surrounding the existing matrix with a padded value. By convention, the padded value is zero.

### Why Padding?

Following is observed in convolution operation

- The corner/edge cells of input matrix are involved in fewer computations.
- The cells in the middle are involved in more computations
- The size of resultant matrix shrinks

The above behavior results in following issues

- The information in corner/edge cells are not utilized to optimum.
- **Shrinking output** $$-$$ With shrinking image applying multiple convolutions becomes impractical.

### Calculation

Let `nxn` be the size of input matrix and `fxf` be the size of the filter and `p` be the padding count.
$$
Output \ Layer \ Size \ = \ (n+2p-f+1, n+2p-f+1)
$$



### Padding to prevent shrinking output

If we have to ensure that the output layer size should be `n` then the required padding can ge figured as follows
$$
\begin{gathered}
n + 2p -f + 1 = n \\
p = \frac{f - 1}{2} \\
\end{gathered}
$$

> Filter is typically odd sized, so padding will typically be symmetric.



## Stride

Shride tells the number of cells to hop (to the right and down) while performing computations. It is analogous to the increment count in a for loop. By default, stride is 1.

![Stride](/assets/images/dl/Stride.png)

Let `nxn` be the size of input matrix and `fxf` be the size of the filter and `p` be the padding count and `s` be the stride.
$$
Output \ Layer \ Size = \left[ \ \left\lfloor {\frac{n+2p-f}{s} + 1}\right\rfloor, \left\lfloor \frac{n+2p-f}{s} + 1 \right\rfloor \ \right]
$$

> The flooring division ensures that the filter boundry does not go out of the input matrix.

# Volume convolution 

## Convolve multiple channels

Volume convolutions are the convolutions performed when the input layer has multple channels (Example: RGB channels). 

> The number of channels in the filter **==** The number of channels in the input layer



![ConvolutionVolume](/assets/images/dl/ConvolutionVolume.png)

Similar to single channel convolutions

- The filter channels are placed over the input layer channels at left hand top corner.
- SInce the number of channels in filter and input layer are the same there is perfect overlap.
- **Sum Of Products(SOP):** The overlapping cells are multiplied and the **entire** volume is summed to get a single digit.
- Thus obtained single digit will be the first element of the resultant layer.
- The filter volume is strided (moved) right and then down to calculate remaining elements.

> Eash cell in the resultant layer is SOP obtained by overlapping filter channels over input channels.

## Understanding channels in filter

Consider a 3 filter channel of RGB.

1. Channel R has vertical edge detector while G and B channels are zeros $$-$$ A volume filter to detect red vertical lines

$$
R=
\begin{bmatrix}
1 & 0 & -1\\
1 & 0 & -1\\
1 & 0 & -1
\end{bmatrix}
\ \ \
G=
\begin{bmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}
\ \ \
B=
\begin{bmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}
$$



2. Channels R, G and B have vertical edge detectors $$-$$ A volume filter to detect vertical lines of any color.

$$
R=
\begin{bmatrix}
1 & 0 & -1\\
1 & 0 & -1\\
1 & 0 & -1
\end{bmatrix}
\ \ \
G=
\begin{bmatrix}
1 & 0 & -1\\
1 & 0 & -1\\
1 & 0 & -1
\end{bmatrix}
\ \ \
B=
\begin{bmatrix}
1 & 0 & -1\\
1 & 0 & -1\\
1 & 0 & -1
\end{bmatrix}
$$



## Multiple filters

A filter (with one or more channels) can detect a feature like red vertical edge, a 45 degree green edge , blue vertical edge and so on. We could have multiple filters as well.

![MultipleFilters](/assets/images/dl/MultipleFilters.png)

In the above diagram

- The input layer was convolved with first filter (yellow) to get a 4x4 output channel.
- The input layer was convolved with second filter (orange) to get another 4x4 output channel.
- The output channels are stacked together to get a 4x4x2 output volume.

> The $$n^{th}$$ channel of the output layer is got by convolving the input layer with the $$n^{th}$$ filter.

## Calculation

| Item                  | Detail                                                       |
| --------------------- | ------------------------------------------------------------ |
| $$n_h$$ x $$n_w$$ x $$n_c$$ | Dimentions of input layer. <br />Note: $$n_c$$ is number of channels in input layer. |
| $$f_h$$ x $$f_w$$ x $$n_c$$ | Dimentions of filter layer. <br />Note: number of filter channels is same as number of input channels |
| $$n_f$$                 | Number of filters                                            |

With the above terminologies for input/filter layers the output layer volume is as follows
$$
Output \ Layer \ Volume = \left[ \ \left\lfloor {\frac{n_h+2p-f_h}{s} + 1}\right\rfloor, \left\lfloor \frac{n_w+2p-f_w}{s} + 1 \right\rfloor, n_f \ \right]
$$

# Types of layers in convolution network

One layer from input 

- Convolution
- Pool 
- Fully Connected

# One Convolution Layer

One convolutional layer refers to 

## Generic Notations

| Item        | Dimension                                     | Detail                                                       |
| ----------- | --------------------------------------------- | ------------------------------------------------------------ |
| $$n^{l-1}_c$$ | -                                             | Number of channels in input layer $$l-1$$                      |
| $$n^l_c$$     | -                                             | Number of channels in output layer $$l$$. This is the same as number of filters in layer $$l$$. |
| $$a_{l-1}$$   | $$n^{l-1}_h$$ x $$n^{l-1}_w$$ x $$n^{l-1}_c$$       | Input volume                                                 |
| $$W_{l}$$     | ( $$f^l_h$$ x $$f^l_w$$ x $$n^{l-1}_c$$ ) x $$n^l_c$$ | Number of channels in a filter volume = Number of channels in input layer<br />Weight of layer $$l$$ = filter volume x  number of filters. |
| $$a_l$$       |                                               | See calculation section of "Multiple filters" above.         |

## Single convolution layer

- Output volume $$z_l$$ is obtained by convolving $$a_{l-1}$$ with $$n^l_c$$  filters each of dimension $$f^l_h$$ x $$f^l_w$$ x $$n^{l-1}_c$$
- A bias is added to each channel of $$z_l$$ and then a activation function is applied. $$a_l = g(z_l + bias \ per \ channel)$$

# One Pooling Layer

Like convolve operation, pooling operation has a filter size. Unlike convolution, the filter *does not contain elements* $$-$$ Think of it as a transparent blank filter. Some operation is performed (SOP in case of convolution) on the overlap to get the result.

## Max Pooling

In case of Max pooling the max among the overlapping elements is used to get the result. The filter strides by stride amount (1 in this case) and the max in the new overlap is calculated. This continues from left to right, top to bottom similar to convolution.

![MaxPool](/assets/images/dl/MaxPool.png)

## Average Pooling

Similar to max pooling instead of the max function the average (mean) is used. Average pooling is not used frequently. 

# Why Convolution?



## Weight (Parameter) Sharing

Consider a 32 x 32 x 3 (3072 neurons) image convolving with 6 filters each of size 5 x 5. The output layer shall be 28 x 28 x 6 (4704 neurons)

- If this was a fully connected NN then W1 = 4704 * 3072 = 14.45 million neurons
- In case of CNN, W1 = 5 * 5 * 3 * 6 = 450

In case of NN, 14 million weights (parameters) are required for a resolution as low as 32 x 32 for a single layer. With higher resolutions this number shall grow to billions.

The reason CNN reduces the weight or parameters is because once a filter detects a feature, *the same filter is applied at different portions of the image*. For example, the same vertical edge detector (a eye/node filter is an example of high level feature) is applied to different sections of the image.

## Sparsity of Connections

**Consider a single output layer result:** A filter volume overlaps over the input volume. Only the overlapping region determines the value of the result in the output layer. The entire non-overlapping section does not involve. This means all neurons in the non-overlapping regions are not connected to produce this output.  
