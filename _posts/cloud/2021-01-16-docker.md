---
title: Docker
categories: cloud
layout: post
mathjax: true
typora-root-url: ../../
---

{% include toc.html %}



# Install

Follow the instructions in [Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu/)

Verify Install

```bash
# Short version
> docker --version
Docker version 20.10.1, build 831ebea

# Detailed version
> docker version                                                                                                                                                                          ✔
Client: Docker Engine - Community
 Version:           19.03.13
 API version:       1.40
 ...
 ...

Server: Docker Engine - Community
 Engine:
  Version:          19.03.13
  API version:      1.40 (minimum version 1.12)
 ...
 ...

 # Help -- Note commads are grouped into 'Management Commands' and 'Commands'
 > docker help
 ...

 Management Commands:
 ...
 ...

 Commands:
```



# Terminologies

| Term             | Detail                                                       |
| ---------------- | ------------------------------------------------------------ |
| Docker Container | A container is a standard unit of software that packages up code and all its dependencies, so the application (app) runs quickly and reliably from one  computing environment to another. |
| Docker Image     | A Docker image is a lightweight, standalone, executable  package of software that includes everything needed to run an  application: code, runtime, system tools, system libraries and settings. |
| Docker Hub       | A Docker hub (https://hub.docker.com) is repository of docker images. |
| Docker Engine    | A Docker engine is a service (daemon process) that runs on a host. <br />A Docker service is started using `sudo systemctl start docker` |
| Docker Client    | The docker commands are executed using `/usr/bin/docker` binary. This is the docker client. <br />The docker client connects with a docker engine (that could be running on local or remote host) to perform operations specified. |

## Understanding using a workflow

**Steps for running a software**

- We check to see if we have an existing setup binary for the desired version we want to install.
  - If found we use the existing setup binary
  - If not, we shall download the setup from the internet.
- We install the software which shall extract the runtime, libraries, dependencies etc into a location on the disk.
- We start the software which shall launch it.

**An analogous docker workflow**

- We check to see if we have an existing docker image for the desired version we want to install. ( `docker image ls`)
  - If found, we shall use the existing image
  - If not, we shall download the setup from the [docker-hub](https://hub.docker.com) (`docker image pull [<userid>/]<image-name>[:<version-tag>]`)
  - Note that a docker image is of the format `[<userid>/]<image-name>[:<version-tag>]`
    - `userid`: The docker login ID of the user who created the image. This is empty for official docker images.
    - `image-name`: The name of the docker image
    - `tag`: A tag differentiates two images and is typically a version string.
- We install the image to create a container (`docker container create --name <container-name> <image> `)
- We list containers. (`docker container ls -a`)
- We start container. (`docker container start <contianer-name> `)

From the above analogy we see that

- A docker image is analogous to setup binary
- A docker-hub is analogous to website hosting setup binary
- A docker container is analogous to a software installed on disk.

## Docker container

A container is a standard unit of software that packages up code and all its dependencies, so the application (app) runs quickly and reliably from one  computing environment to another.

> An app feels like it running inside a machine of its own with its own resources (RAM, harddisk, CPU and network). A container is analogous to a womb for an app.

# Docker core commands



## Core workflow

The following commands demo a typical docker workflow using which we are introduced to docker image, container

```bash
# Download an image hello-world
> docker image pull hello-world

# List download image
> docker image ls
hello-world           latest    bf756fb1ae65   12 months ago   13.3kB

# Create a container
> docker container create --name my_hello hello-world

# List all containers
> docker container ls -a
CONTAINER ID   IMAGE     COMMAND             CREATED        STATUS          PORTS                     NAMES
d4a7ed8470f9   hello-world  "/hello"         3 weeks ago    Exited (0)                                my_hello

# Start a container
> docker container start my_hello -i
Hello from Docker!
...
```

## Core container operations

### Running a container

A `docker run` command  does all the operations described in the docker workflow.

- If we execute the `docker run` command multiple times we would land up creating multiple copies of the same container with random names (assigned by docker).
- This is similar to installing the same software multiple times in random directories. By providing a name (using `--name`) trying to create another container with same name fails -- This way, we rather re-use an existing container.
- The `--rm` option removes a container after it's stopped. This way we have the flexibility of using `docker run` all the time without piling up replicas of containers.

```dockerfile
# The command has an empty output since the container was started, did its job of saying hello and then shutdown.
# If it were a server like tomcat then the container would be running until it is stopped.
> docker container ls
CONTAINER ID   IMAGE                       COMMAND                  CREATED         STATUS         PORTS     NAMES

# Lets run a tomcat with a volume mapping (we will see later, for now we are mapping a dir in local with that in container)
# We see that we are able get a response to our app
> cp duke.war ~/work/web-app
> cd ~/work/web-app
> docker run --rm --name my_tommy --detach --publish 18801:8080 --volume ~/work/web-app:/usr/local/tomcat/webapps tomcat:9
> curl -s -I http://localhost:18801/duke/Snoop.jsp | head -1
HTTP/1.1 200

# List running containers
> docker container ls
CONTAINER ID   IMAGE      COMMAND             CREATED              STATUS              PORTS                     NAMES
64326d1e2f4d   tomcat:9   "catalina.sh run"   About a minute ago   Up About a minute   0.0.0.0:18801->8080/tcp   my_tommy

```

### Start, stop and check container logs

```bash
# Stop container
> docker container stop my_tommy
my_tommy

# Start container
> docker container start my_tommy

# Show the last 3 lines from logs
> docker container logs -n 3 my_tommy
21-Jan-2021 06:20:48.369 INFO [main] org.apache.catalina.startup.HostConfig.deployDirectory Deployment of web application directory [/usr/local/tomcat/webapps/duke] has finished in [718] ms
21-Jan-2021 06:20:48.376 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
21-Jan-2021 06:20:48.414 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in [865] milliseconds

```

### Login into a running container

```bash
> docker container exec -it my_tommy sh
# hostname
4286b5cec5b8
# cd /usr/local/tomcat
```

### Cleanup

```bash
# Remove containers
docker container rm <container name ...>

# Remove images
docker image rm <image id ...>

# Remove dangling images (no container is referencing them)
docker image prune
```

# Playing with docker run

## Remove container after exit

```bash
# Run a command on the container. Remove container when done.
> docker container run --rm cafeduke/busybox cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.2	a37c77eb628d
```

## Interactive shell
```bash
# Get an interactive shell to a container
docker container run --rm -it cafeduke/busybox sh -l
Sourcing /root/.profile
172.17.0.2 /> hostname
0be6321def2b
172.17.0.2 />
```

# Docker Image

A custom docker image can be built in one of the following ways. The default name of the file having build instructions is `Dockerfile`

```bash
# File <path-to-build-dir>/Dockerfile is the file having build instructions
docker build <path-to-build-dir> --tag <tag-name>

# File <path-to-file> is the file having build instructions
docker build --file <path-to-file> --tag <tag-name>
```

## Dockerfile v1

A Dockerfile consists of a **directive** like `FROM, RUN, WORKDIR` etc and corresponding arguments as documented in [docs.docker.com](https://docs.docker.com/engine/reference/builder/#parser-directives)

```bash
> cd Learn/Docker/01-image/image-hello
> cat Dockerfile

# 1. Use alpine as the base
FROM alpine:latest

# 2. Run update and install bash
RUN /bin/sh -c 'apk update; \
apk add bash'

# 3. Change pwd to /home
WORKDIR /home

# 4. Copy arg.sh to WORKDIR
COPY arg.sh .

# 5. Set entry point as shell script
ENTRYPOINT ["/home/arg.sh"]

# 6. Supply HelloWorld as the argument
CMD ["HelloWorld!"]
```

### Build a fresh image

- Docker builds an image incrementally. Each **step** corresponds to the directive and its arguments. For example, the above `Dockerfile` has 6 directives that corresponds to 6 steps.
- After completing `Step1` the resultant **layer** (partial image) is cached.
- After completing `Step2` the resultant layer which is an aggregate of performing directives in step 1 and 2 is also cached.
- After completing `Step3` the resultant layer which is an aggregate of performing directives in step 1, 2 and 3 is also cached and so on.

```bash
# Build a fresh image and note the steps
# --------------------------------------
> docker image build . --tag raghubs81/test-entrypoint
Sending build context to Docker daemon  16.38kB

Step 1/6 : FROM alpine:latest
latest: Pulling from library/alpine
Digest: sha256:08d6ca16c60fe7490c03d10dc339d9fd8ea67c6466dea8d558526b1330a85930
Status: Downloaded newer image for alpine:latest
 ---> e50c909a8df2

Step 2/6 : RUN /bin/sh -c 'apk update; apk add bash'
 ---> Running in 535d373026c8
fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz
fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz
v3.13.1-78-gb575f39b62 [https://dl-cdn.alpinelinux.org/alpine/v3.13/main]
v3.13.1-79-g25eb590247 [https://dl-cdn.alpinelinux.org/alpine/v3.13/community]
OK: 13878 distinct packages available
(1/4) Installing ncurses-terminfo-base (6.2_p20210109-r0)
(2/4) Installing ncurses-libs (6.2_p20210109-r0)
(3/4) Installing readline (8.1.0-r0)
(4/4) Installing bash (5.1.0-r0)
Executing bash-5.1.0-r0.post-install
Executing busybox-1.32.1-r2.trigger
OK: 8 MiB in 18 packages
Removing intermediate container 535d373026c8
 ---> 6b57a2f9e0b7

Step 3/6 : WORKDIR /home
 ---> Running in 0a2c88268315
Removing intermediate container 0a2c88268315
 ---> 566d0edc5325

Step 4/6 : COPY arg.sh .
 ---> 641ce9651e38

Step 5/6 : ENTRYPOINT ["/home/arg.sh"]
 ---> Running in 02868bd4468a
Removing intermediate container 02868bd4468a
 ---> 6a811329a8b2

Step 6/6 : CMD ["HelloWorld!"]
 ---> Running in 54137bb598bc
Removing intermediate container 54137bb598bc
 ---> 91572d7dbec3

Successfully built 91572d7dbec3
Successfully tagged raghubs81/test-entrypoint:latest

> docker image ls
REPOSITORY                  TAG       IMAGE ID       CREATED         SIZE
raghubs81/test-entrypoint   latest    91572d7dbec3   3 minutes ago   9.74MB
```



### Rebuilding the image

When image is rebuilt relevant layer (if available)  is picked from cache.

```bash
# Rebuild image and note the steps
# --------------------------------------
> docker image build . --tag raghubs81/test-entrypoint
Sending build context to Docker daemon  16.38kB
Step 1/6 : FROM alpine:latest
 ---> e50c909a8df2

Step 2/6 : RUN /bin/sh -c 'apk update; apk add bash'
 ---> Using cache
 ---> 6b57a2f9e0b7

Step 3/6 : WORKDIR /home
 ---> Using cache
 ---> 566d0edc5325

Step 4/6 : COPY arg.sh .
 ---> Using cache
 ---> 641ce9651e38

Step 5/6 : ENTRYPOINT ["/home/arg.sh"]
 ---> Using cache
 ---> 6a811329a8b2

Step 6/6 : CMD ["HelloWorld!"]
 ---> Using cache
 ---> 91572d7dbec3

Successfully built 91572d7dbec3
Successfully tagged raghubs81/test-entrypoint:latest
```

## Dockerfile v2

The version 2 of the `Dockerfile` adds a new line as `Step5`

```bash
# 1. Use alpine as the base
FROM alpine:latest

# 2. Run update and install bash
RUN /bin/sh -c 'apk update; \
apk add bash'

# 3. Change pwd to /home
WORKDIR /home

# 4. Copy arg.sh to WORKDIR
COPY arg.sh .

# 5. Set environment variable
ENV MY_FRUIT=Mango

# 6. Set entry point as shell script
ENTRYPOINT ["/home/arg.sh"]

# 7. Supply HelloWorld as the argument
CMD ["HelloWorld!"]
```



```bash
# Note that step #1-4 is taken from cache
# ---------------------------------------

Sending build context to Docker daemon  19.46kB
Step 1/7 : FROM alpine:latest
 ---> e50c909a8df2

Step 2/7 : RUN /bin/sh -c 'apk update; apk add bash'
 ---> Using cache
 ---> 6b57a2f9e0b7

Step 3/7 : WORKDIR /home
 ---> Using cache
 ---> 566d0edc5325

Step 4/7 : COPY arg.sh .
 ---> Using cache
 ---> 641ce9651e38

# Note that step #5 onwards cannot be picked from cache
# -----------------------------------------------------
Step 5/7 : ENV MY_FRUIT=Mango
 ---> Running in ea8ce6432a5d
Removing intermediate container ea8ce6432a5d
 ---> df5c8d46ba8d

Step 6/7 : ENTRYPOINT ["/home/arg.sh"]
 ---> Running in ed8fb46999b1
Removing intermediate container ed8fb46999b1
 ---> a59793774e20

Step 7/7 : CMD ["HelloWorld!"]
 ---> Running in 7638301279db
Removing intermediate container 7638301279db
 ---> d19781e107ce

Successfully built d19781e107ce
Successfully tagged raghubs81/test-entrypoint:latest
```



> If the **n**th directive in a docker file  is modified then only n-1 layers can be picked from cache.

# Data persistence

Persistence of data is achieved by either using volumes or bind mount. However, bind mount is used only during development. Docker volumes is the persistence mechanism in production.

## Why persist?

The data generated by the container is lost when the container is removed. However, in several cases (like database, configuration of server, blogs hosted by server) we would want to retain the data. This can be achieved using persistence.

## Docker volumes

### Unnamed volumes

```bash
# Build a custom image (We will see more details later)
cd Learn/Docker/02-volume/volume-tomcat
> docker build . --tag cafeduke/tomcat:latest

# Run the tomcat with custom app (Do NOT use --rm)
> docker run --name duke_tommy --detach --publish 18801:8080 cafeduke/tomcat:latest
> curl -s "http://localhost:18801/duke/"
             ___________
            /           |
      __   (   Welcome! |
  ___( o)>  \___________|
  \ <_. )
   `---'
# Inspect container
#   - The webapps at '/usr/local/tomcat/webapps' reffered by container are actually mounted at /var/lib/docker/volumes/14a5...440f/_data
# 	- The mount type is 'volume'
> docker inspect duke_tommy | grep -A 10 Mounts
        "Mounts": [
            {
                "Type": "volume",
                "Name": "14a5c567cde9318ad0cd5fe8787abcb093bf97483841494937979872af7d440f",
                "Source": "/var/lib/docker/volumes/14a5c567cde9318ad0cd5fe8787abcb093bf97483841494937979872af7d440f/_data",
                "Destination": "/usr/local/tomcat/webapps",
                "Driver": "local",
                "Mode": "",
                "RW": true,
                "Propagation": ""
            }

# Note that the volume exists
> docker volume ls
DRIVER    VOLUME NAME
local     5b251e51b2c59c86ca6e4045d234c9061fced260fda769e40a47769143080450
local     10a6711080b655fb3d062c11e4bf3d5af9c9b63722caa65a3c6446ecb3125bca
local     14a5c567cde9318ad0cd5fe8787abcb093bf97483841494937979872af7d440f

# Stop  and remove the container
> docker stop duke_tommy; docker rm duke_tommy

# Ensure volume exists after container is removed
docker volume ls | grep 14a5c567cde9318ad0cd5fe8787abcb093bf97483841494937979872af7d440f
local     14a5c567cde9318ad0cd5fe8787abcb093bf97483841494937979872af7d440f
```

### Named volumes

Named volumes cannot be created by the container itself (As part of Dockerfile) and needs to be explicitly specified by the user using  `docker container run --volume <name>:<path in container>`. This is because for a given host, the name of the volume is unique. So, it is up to the user to ensure providing a unique volume name.

```bash
# Build image
cd Learn/Docker/02-volume/volume-tomcat
> docker build . --tag cafeduke/tomcat:latest

# Start container with named volume
> docker run --name duke_tommy --detach --publish 18801:8080 --volume duke_tommy_volume:/usr/local/tomcat/webapps cafeduke/tomcat:latest

# Note that volume exists
docker volume ls | grep tommy
local     duke_tommy_volume

# Stop  and remove the container
> docker stop duke_tommy; docker rm duke_tommy

# Ensure volume exists after container is removed
docker volume ls | grep tommy
local     duke_tommy_volume

```

## Bind mount

A bind mount maps a directory in the host to a directory in the container.

	- Bind mount is only for development  -- A container cannot be programmed (via Dockerfile) to bind mount.
	- Bind mount is very useful during development -- For example, change app server pages, refresh the browser page to check working.

### Bind mount tomcat app

```bash
# Creat a dir and copy the .war file to be deployed
> mkdir ~/work/test-bind-mount
> cp duke.war ~/work/test-bind-mount
> cd ~/work/test-bind-mount

# Map the current directory to the webapps dir of tomcat
> docker run --name duke_tommy --detach --publish 18801:8080 --volume $(pwd):/usr/local/tomcat/webapps cafeduke/tomcat:latest

# We are able do dynamically add files to duke and see changes.
> echo "Hello" > hello.txt
> sudo mv hello.txt DukeApp
> curl -s "http://localhost:18801/duke/hello.txt"
Hello
```

### Bind mount GitHub website

```bash
# Access the jekyll website at localhost:8080
> git clone https://github.com/cafeduke/cafeduke.github.io DukeWebSite
> cd DukeWebSite
> docker container run --rm --name duke_site --detach --publish 8080:4000 --volume $(pwd):/site bretfisher/jekyll-serve
```

# Docker Network

One of the reasons Docker containers and services are so powerful is that you can connect them together. Docker’s networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality.

>  A network:container relationship is m:n -- A network can have multiple containers connected to it. A container can be connected to multiple networks.

| Network Driver | Detail                                                       |
| -------------- | ------------------------------------------------------------ |
| Bridge         | The default network driver. If you don’t specify a driver, this is the type of network you are creating.<br />Bridge network allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network.<br />**User-defined bridges provide automatic DNS resolution between containers** |
| Overlay        | The `overlay` network driver creates a distributed network among multiple Docker daemon hosts. This network sits on top of (overlays) the host-specific networks.<br />Services or containers can be connected to more than one network at a time. Services or containers can only communicate across networks they are each connected to. |

## Bridge network

### Default

```bash
> docker container run --rm --tty --detach --name my_box.1 cafeduke/busybox
> docker container run --rm --tty --detach --name my_box.2 cafeduke/busybox

# Inspect default network 'bridge' to find the IP of the containers
> docker inspect bridge | egrep "Name|IPv4"
"Name": "bridge",
  "Name": "my_box.1",
  "IPv4Address": "172.17.0.2/16",
  "Name": "my_box.2",
  "IPv4Address": "172.17.0.3/16",

# Attach to a container -- Verify communcation b/w containers in SAME DEFAULT network using IP works.
> docker container exec -it my_box.1 sh -l
172.17.0.2 /> ping -c 2 172.17.0.3
PING 172.17.0.3 (172.17.0.3): 56 data bytes
64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.166 ms
64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.200 ms

# Attach to a container -- Verify communcation b/w containers in SAME DEFAULT network using hostname FAILS.
172.17.0.2 /> ping -c 2 my_box.2
ping: bad address 'my_box.2'
```

### Custom

```bash
# Create two networks
> docker network create --driver bridge my-net-bridge-1
> docker network create --driver bridge my-net-bridge-2

# Attach two containers to my-net-bridge network.
# Attach third container to my-net-bridge-2 network
> docker container run --rm --tty --detach --name my_box.1 --network my-net-bridge-1 cafeduke/busybox
> docker container run --rm --tty --detach --name my_box.2 --network my-net-bridge-1 cafeduke/busybox
> docker container run --rm --tty --detach --name my_box.3 --network my-net-bridge-2 cafeduke/busybox

# Attach to a container -- Verify communcation b/w containers in SAME CUSTOM network using IP works.
> docker container exec -it my_box.1 sh -l
172.18.0.2 /> ping -c 2 172.18.0.3
64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.193 ms
64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.169 ms

# Attach to a container -- Verify communcation b/w containers in SAME CUSTOM network using hostname works.
172.18.0.2 /> ping -c 2 my_box.2
64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.109 ms
64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.173 ms

> docker inspect network my-net-bridge-2 | egrep "Name|IPv4"
"Name": "my-net-bridge-2",
  "Name": "my_box.3",
  "IPv4Address": "172.19.0.2/16",

# Attach to a container -- Verify communcation b/w containers in DIFFERENT CUSTOM network using IP/hostname FAILS.
172.18.0.2 /> ping -c 2 my_box.3
ping: bad address 'my_box.3'

> ping -c 2 172.19.0.2
PING 172.19.0.2 (172.19.0.2): 56 data bytes
2 packets transmitted, 0 packets received, 100% packet loss
```

## Overlay network

Overlay network comes into play in docker swarm as we shall see later.

> Before you can create an overlay network, you need to either initialize your Docker daemon as a swarm manager using `docker swarm init` or join it to an existing swarm using `docker swarm join`.
>
> Either of these creates the default `ingress` overlay network which is used by swarm services by default. You need to do this even if you never plan to use swarm services. Afterward, you can create additional user-defined overlay networks.

```bash
# Create two attachable overlay network
> docker network create --driver overlay --attachable my-net-overlay-1
> docker network create --driver overlay --attachable my-net-overlay-2

# Attach my_box.1 to network 'my-net-overlay' and 'my_box.2' to 'my-net-overlay-test'
> docker container run --rm --detach -it --name my_box.1 --network my-net-overlay-1 cafeduke/busybox
> docker container run --rm --detach -it --name my_box.2 --network my-net-overlay-2 cafeduke/busybox

# Attach to a container -- Verify communcation b/w containers in DIFFERENT CUSTOM OVERLAY network using IP/hostname FAILS.
> docker container exec -it my_box.1
172.21.0.2 /> ping -c 2 my_box.2
ping: bad address 'my_box.2'
172.21.0.2 /> Ctrl-pq (detach)

```



# Swarm single node

A docker swarm performs **container orchestration** across multiple nodes. Swarm nodes could be either master nodes or worker nodes.

> A node is a machine (could be a VM)

A swarm is a group of nodes that have joined as either manager node or worker node.
  - A swarm must have at-least one manager.
  - A new node can either join as manager or worker.
  - The status of a node (manager/worker) can be change.

A swarm has security automation
  - Root Signing certificate is created for the Swarm.
  - First manager node is issued a certificate.
  - Different join tokens are created for nodes to join as worker.
  - Different join tokens are created for nodes to join as manager.

## Managing swarm

### Initialise a swarm

```bash
# Initialize swarm and get command to addes nodes as workers
> docker swarm init
Swarm initialized: current node (oo75je2p82824595oznd9tnaf) is now a manager.
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-1l9bzy4gyvtfhonoug854zdkt296lyalk6chjco40jms0m2nt9-63kvy0s9ocpbgu2e3cuo1d8k4 192.168.0.107:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

# Get command to to add nodes as managers
> docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-1l9bzy4gyvtfhonoug854zdkt296lyalk6chjco40jms0m2nt9-8qs4tcpdispl1d2tamg7bz2l9 192.168.0.107:2377

```

- This adds the current node to the swarm.
- Since this is the only node -- It is now the master.
- The `docker swarm join ...` command that follows can be run on other nodes, so that they can **join the swarm as worker**
- If we want other nodes to join as a manager as well then we run `docker swarm join-token manager` -- This generates a join command with different token.

In essence, we have initialised a swarm and have the join commands to add nodes as either manager or worker.

### Join a swarm

As seen in the above section `docker swarm join ...` command is used to join a swarm as either manager or worker. However, note that a worker can be promoted and manager can be demoted, later as well!

### Leave a swarm

To leave a swarm execute `docker swarm leave` on the corresponding node.

## Managing nodes

The swarm nodes are managed using `docker node` command

### List nodes

List all the nodes of this swarm. Since this is a single node swarm -- we will have just one node. Note that the manager status is **Leader**

```bash
> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
oo75je2p82824595oznd9tnaf *   raghu-pc   Ready     Active         Leader           20.10.1
```

### Promote/demote nodes

Worker nodes can be promoted later by running `docker node promote <node-id...>` on the node.
Manager nodes can be demoted later by running `docker node demote <node-id...>` on the node.

### List tasks running a node

The `docker node ps [<node-id...>]` command is used to list **tasks** running on one or more nodes (defaults to current node). Docker tasks are covered later.

# Swarm service

A docker service exists only in the **swarm world**. A docker service is an abstraction (Similar to Kubernetes cluster) that is used manage a homogeneous cluster of containers.

  - A docker service is a feature that exists **only** in the context of swarm.
  - A docker service can be used to create replicas of tasks, **scale-up** tasks and **scale-down** tasks.
  - Each task is a wrapper for a docker container.
  - If a task/container gets killed (Eg: node hosting the container goes down) then a swarm service automatically starts the task on another node.

> A docker service can be used to scale the cluster  -- Scale up/down tasks across nodes.
> A docker service is also maintains the cluster size -- If task gets killed it is started on another node.

## Core service operations

The below commands demonstrate the advantage of docker service.

### Create service

The `docker service create` command is very similar to the `docker container run` command. Here we created a service with `alpine` image and provided it with the task of `pinging 8.8.8.8`. The `8.8.8.8` is a google DNS IP which shall always be there and ping command never returns in Unix. So, the container shall run for ever.

```bash
> docker service create --name my_box --tty --detach cafeduke/busybox:latest sh -l
u3vu6onvmb6w9mxckdooy8c5v
```

### List services

```bash
# List services
> docker service ls
ID             NAME      MODE         REPLICAS   IMAGE                     PORTS
u3vu6onvmb6w   my_box    replicated   1/1        cafeduke/busybox:latest

# List tasks for a given service
#  - Task name is of the format <service-name>.<task-index>
#  - Task could be running in any node. However, we have only one node here.
> docker service ps my_box
ID             NAME       IMAGE                     NODE       DESIRED STATE   CURRENT STATE            ERROR     PORTS
5vif6wy9ityo   my_box.1   cafeduke/busybox:latest   raghu-pc   Running         Running 46 seconds ago

# List container (Wrapped by task)
#  - A task wraps a container. Hence TaskId is NOT the same as ContainerId
#  - The name of the container wrapped by the task is of the format <service-name>.<task-index>.<task-id>
> docker container ls
CONTAINER ID   IMAGE                     COMMAND   CREATED              STATUS              PORTS     NAMES
3aae4f88ddf4   cafeduke/busybox:latest   "sh -l"   About a minute ago   Up About a minute             my_box.1.5vif6wy9ityod8xushbs400ew
```

### Scale up

```bash
# Scale up  -- The service from 1 to 5
> docker service scale my_box=5
my_box scaled to 5
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service converged

# Note that all 5 replicas are running
> docker service ls
ID             NAME      MODE         REPLICAS   IMAGE                     PORTS
u3vu6onvmb6w   my_box    replicated   5/5        cafeduke/busybox:latest

# We have 3 tasks (my_alpine.1 through my_alpine.3)
> docker service ps my_box
ID             NAME       IMAGE                     NODE       DESIRED STATE   CURRENT STATE            ERROR     PORTS
5vif6wy9ityo   my_box.1   cafeduke/busybox:latest   raghu-pc   Running         Running 3 minutes ago
jfevqsv77svc   my_box.2   cafeduke/busybox:latest   raghu-pc   Running         Running 38 seconds ago
eih37w3i9t7w   my_box.3   cafeduke/busybox:latest   raghu-pc   Running         Running 38 seconds ago
w49sb4f8jcij   my_box.4   cafeduke/busybox:latest   raghu-pc   Running         Running 39 seconds ago
w7pob4xku6cx   my_box.5   cafeduke/busybox:latest   raghu-pc   Running         Running 38 seconds ago
```

### Scale down

```bash
# Scale down  -- The service from 3 to 2
> docker service scale my_box=2
my_box scaled to 2
overall progress: 2 out of 2 tasks
1/2: running   [==================================================>]
2/2: running   [==================================================>]
verify: Service converged

> docker service ls
ID             NAME      MODE         REPLICAS   IMAGE                     PORTS
u3vu6onvmb6w   my_box    replicated   2/2        cafeduke/busybox:latest
```

### Task resilience

If a container goes down, the service ensures it is restarted.

```bash
# The service has two tasks and are they are running.
> docker service ps my_box
ID             NAME       IMAGE                     NODE       DESIRED STATE   CURRENT STATE                ERROR     PORTS
vyvf0mzqrsvb   my_box.1   cafeduke/busybox:latest   raghu-pc   Running         Running about a minute ago
fvjwtgdfdw51   my_box.2   cafeduke/busybox:latest   raghu-pc   Running         Running about a minute ago

# Following are the corresponding containers for the task.
# Note that tasks abstract containers and medling with containers is really backdoor for a task.
> docker container ls | grep my_box
b5eab599c57a   cafeduke/busybox:latest     "sh -l"                  2 minutes ago   Up 2 minutes             my_box.2.fvjwtgdfdw51s0yuk0ygoekot
738fef75a486   cafeduke/busybox:latest     "sh -l"                  2 minutes ago   Up 2 minutes             my_box.1.vyvf0mzqrsvb65hxhu1eqepbo

# Stop a container
> docker container stop my_box.2.fvjwtgdfdw51s0yuk0ygoekot

# Note that the task restarts
# On a multi-node env, the task could have been restarted on any node.
> docker service ps my_box
ID             NAME           IMAGE                     NODE       DESIRED STATE   CURRENT STATE           ERROR                         PORTS
vyvf0mzqrsvb   my_box.1       cafeduke/busybox:latest   raghu-pc   Running         Running 3 minutes ago
kbul3mk69cy4   my_box.2       cafeduke/busybox:latest   raghu-pc   Running         Running 8 seconds ago
fvjwtgdfdw51    \_ my_box.2   cafeduke/busybox:latest   raghu-pc   Shutdown        Failed 13 seconds ago   "task: non-zero exit (137)"

# We could filter out the failed tasks and show only the running ones as follows
> docker service ps my_box --filter desired-state=running
ID             NAME       IMAGE                     NODE       DESIRED STATE   CURRENT STATE           ERROR     PORTS
vyvf0mzqrsvb   my_box.1   cafeduke/busybox:latest   raghu-pc   Running         Running 5 minutes ago
kbul3mk69cy4   my_box.2   cafeduke/busybox:latest   raghu-pc   Running         Running 2 minutes ago
```

### Remove a service

```bash
# Remove all services
# This removes all the assiciated tasks
> docker service rm my_box
```

## Docker service across overlay network

```bash
# Create two overlay network
> docker network create --attachable --driver overlay my-net-overlay-1
> docker network create --attachable --driver overlay my-net-overlay-2

# Create two services attached to DIFFERENT network
> docker service create --name foo_box --replicas=3 --network my-net-overlay-1 --tty --detach cafeduke/busybox:latest sh -l
> docker service create --name bar_box --replicas=2 --network my-net-overlay-2 --tty --detach cafeduke/busybox:latest sh -l

# List containers
> docker container ls
CONTAINER ID   IMAGE                     COMMAND   CREATED              STATUS          PORTS     NAMES
f22126a42a2f   cafeduke/busybox:latest   "sh -l"   37 seconds ago       Up 36 seconds             bar_box.2.t062uy6zqv26etmh2r01m2xae
06c20eddf648   cafeduke/busybox:latest   "sh -l"   37 seconds ago       Up 36 seconds             bar_box.1.mdxen6tsht4ijhuuonnbieemv
5efe31db796f   cafeduke/busybox:latest   "sh -l"   59 seconds ago       Up 58 seconds             foo_box.1.x49cl1czrm14vn6pfn3bo8uqt
6875c9d8ea87   cafeduke/busybox:latest   "sh -l"   About a minute ago   Up 58 seconds             foo_box.2.k8upsje2qnfcejtldabktnbfd
4089040fe4b2   cafeduke/busybox:latest   "sh -l"   About a minute ago   Up 58 seconds             foo_box.3.s13r28pgabx8udbadtmqvdfzr

# Verify that container from one network CANNOT communicate with other.
> docker container exec -it foo_box.1.x49cl1czrm14vn6pfn3bo8uqt sh -l
Sourcing /root/.profile
10.0.1.9 /> ping -c 2 bar_box
ping: bad address 'bar_box'

```

# Swarm stack single node

## Create a stack YAML
- The image used here is 'hypoport/httpd-cgi' which is an Apache HTTP server with CGI scripts
- We have ap_foo service and ap_bar service with multiple replicas

> Both services have a common network 'my_net' due to which they can communicate with each other using the SERVICE name as DNS

```bash
> cat docker-compose.yml
version: "3.8"

services:

  ap_foo:
    image: hypoport/httpd-cgi
    ports:
      - 8801:80
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    networks:
      - my_net_foo
      - my_net

  ap_bar:
    image: hypoport/httpd-cgi
    ports:
      - 8802:80
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    networks:
      - my_net_bar
      - my_net

networks:
  my_net_foo:
  my_net_bar:
  my_net:
```
## Deploy, list services, tasks and containers
```bash
# Deploy the stack using compose file
> docker stack deploy --compose-file docker-compose.yml mystack
Creating network mystack_my_net_bar
Creating network mystack_my_net_foo
Creating network mystack_my_net
Creating service mystack_ap_foo
Creating service mystack_ap_bar

# List the swarms
> docker stack ls
NAME      SERVICES   ORCHESTRATOR
mystack   2          Swarm

# List services of a given swarm
> docker stack services mystack
ID             NAME             MODE         REPLICAS   IMAGE                       PORTS
udr6qt5bwvjc   mystack_ap_bar   replicated   2/2        hypoport/httpd-cgi:latest   *:8802->80/tcp
nc2w6f22eee8   mystack_ap_foo   replicated   3/3        hypoport/httpd-cgi:latest   *:8801->80/tcp

# List tasks of a given swarm
> docker stack ps mystack
ID             NAME               IMAGE                       NODE       DESIRED STATE   CURRENT STATE           ERROR     PORTS
y8nf79qw9imm   mystack_ap_bar.1   hypoport/httpd-cgi:latest   raghu-pc   Running         Running 4 minutes ago
kc8twgfo8skk   mystack_ap_bar.2   hypoport/httpd-cgi:latest   raghu-pc   Running         Running 4 minutes ago
ihzapy3kdzge   mystack_ap_foo.1   hypoport/httpd-cgi:latest   raghu-pc   Running         Running 4 minutes ago
ix6ix23100yi   mystack_ap_foo.2   hypoport/httpd-cgi:latest   raghu-pc   Running         Running 4 minutes ago
7hxvf0223elq   mystack_ap_foo.3   hypoport/httpd-cgi:latest   raghu-pc   Running         Running 4 minutes ago

# List all containers
> docker container ls
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS     NAMES
5aba92fbd850   hypoport/httpd-cgi:latest   "/usr/local/apache2/…"   6 seconds ago    Up 3 seconds    80/tcp    mystack_ap_bar.2.kc8twgfo8skk6eknnhhm661te
4927e02f38e0   hypoport/httpd-cgi:latest   "/usr/local/apache2/…"   6 seconds ago    Up 3 seconds    80/tcp    mystack_ap_bar.1.y8nf79qw9immrlu4ekk2h6fm9
b89c7a2a3a36   hypoport/httpd-cgi:latest   "/usr/local/apache2/…"   11 seconds ago   Up 9 seconds    80/tcp    mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek
498ff86251e2   hypoport/httpd-cgi:latest   "/usr/local/apache2/…"   11 seconds ago   Up 9 seconds    80/tcp    mystack_ap_foo.3.7hxvf0223elqxyy6fxqzpyzyq
f55d7e926592   hypoport/httpd-cgi:latest   "/usr/local/apache2/…"   11 seconds ago   Up 9 seconds    80/tcp    mystack_ap_foo.2.ix6ix23100yi0c9zqy1ad5hgg
```

## Verify tasks communicate using service name

```bash
# mystack_ap_foo.1 can communicate with one of the tasks among ap_bar using the SERVICE name 'ap_bar' as hostname
> docker exec mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek curl -s http://ap_bar/hello
Hello world!

# mystack_ap_bar.1 can communicate with one of the tasks among ap_foo using the SERVICE name 'ap_foo' as hostname
> docker exec mystack_ap_bar.1.y8nf79qw9immrlu4ekk2h6fm9 curl -s http://ap_foo/hello
Hello world!
```

## Verify routing mesh

### IPs from networks

Each network the container is associated with gives it an IP.

> A network acts like an NIC (Network Interface Card) for the container.

```bash
> docker network ls | grep mystack
y33ke14moxcw   mystack_my_net       overlay   swarm
vndr6wduxwx9   mystack_my_net_bar   overlay   swarm
i4c3ifvcii4d   mystack_my_net_foo   overlay   swarm

# IPs of ap_foo and ap_bar containers from shared network
> docker network inspect mystack_my_net | egrep "Name|IPv4"
"Name": "mystack_my_net",
  "Name": "mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek",
  "IPv4Address": "10.0.9.3/24",
  "Name": "mystack_ap_foo.2.ix6ix23100yi0c9zqy1ad5hgg",
  "IPv4Address": "10.0.9.4/24",
  "Name": "mystack_ap_foo.3.7hxvf0223elqxyy6fxqzpyzyq",
  "IPv4Address": "10.0.9.5/24",

  "Name": "mystack_ap_bar.1.y8nf79qw9immrlu4ekk2h6fm9",
  "IPv4Address": "10.0.9.8/24",
  "Name": "mystack_ap_bar.2.kc8twgfo8skk6eknnhhm661te",
  "IPv4Address": "10.0.9.9/24",

  "Name": "mystack_my_net-endpoint",
  "IPv4Address": "10.0.9.6/24",

# IPs of ap_foo containers
> docker network inspect mystack_my_net_foo | egrep "IPv4"
"IPv4Address": "10.0.8.5/24",
"IPv4Address": "10.0.8.3/24",
"IPv4Address": "10.0.8.4/24",
"IPv4Address": "10.0.8.6/24",

# IPs of ap_bar containers
> docker network inspect mystack_my_net_bar | egrep "IPv4"
"IPv4Address": "10.0.7.3/24",
"IPv4Address": "10.0.7.4/24",
"IPv4Address": "10.0.7.5/24",

```

### Verify load balancing

>  Requesting one service from another, using service name as hostname, automatically load-balances (round robin algorithm) among the available tasks.


```bash
# Request service ap_foo from one of bar containers
docker container exec -it mystack_ap_bar.1.y8nf79qw9immrlu4ekk2h6fm9 bash -c 'for x in {1..10}
> do
>   curl -s http://ap_foo/show_env | grep SERVER_ADDR
> done
> '
SERVER_ADDR=10.0.9.5
SERVER_ADDR=10.0.9.4
SERVER_ADDR=10.0.9.3
SERVER_ADDR=10.0.9.5
SERVER_ADDR=10.0.9.4
SERVER_ADDR=10.0.9.3
SERVER_ADDR=10.0.9.5
SERVER_ADDR=10.0.9.4
SERVER_ADDR=10.0.9.3
SERVER_ADDR=10.0.9.5

# Request service ap_bar from one of foo containers
> docker container exec mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek bash -c 'for x in {1..10}
> do
>   curl -s http://ap_bar/show_env | grep SERVER_ADDR
> done
> '
SERVER_ADDR=10.0.9.9
SERVER_ADDR=10.0.9.8
SERVER_ADDR=10.0.9.9
SERVER_ADDR=10.0.9.8
SERVER_ADDR=10.0.9.9
SERVER_ADDR=10.0.9.8
SERVER_ADDR=10.0.9.9
SERVER_ADDR=10.0.9.8
SERVER_ADDR=10.0.9.9
SERVER_ADDR=10.0.9.8
```

### Verify VIP

- The IPs obtained by pinging services `ap_bar` and `ap_foo` do not belong to any container!
- These are VIPs -- There is no container running with this IP  -- The swarm manages these IPs
- Tasks/Containers may die and come up with another IP --  The corresponding VIP will keep track.
- The VIP remains and load-balances among the tasks of the service.

```bash
> docker exec mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek ping -c 2 ap_bar
PING ap_bar (10.0.9.7): 56 data bytes
64 bytes from 10.0.9.7: seq=0 ttl=64 time=0.088 ms
64 bytes from 10.0.9.7: seq=1 ttl=64 time=0.182 ms

> docker exec mystack_ap_foo.1.ihzapy3kdzgeskl6qqpm0voek ping -c 2 ap_foo
PING ap_foo (10.0.9.2): 56 data bytes
64 bytes from 10.0.9.2: seq=0 ttl=64 time=0.099 ms
64 bytes from 10.0.9.2: seq=1 ttl=64 time=0.165 ms

# Note that the address returned by the ping to service is not part of any network container
> docker network inspect mystack_my_net mystack_my_net_foo mystack_my_net_bar | grep 10.0.9.2
> docker network inspect mystack_my_net mystack_my_net_foo mystack_my_net_bar | grep 10.0.9.7

```

## Cleanup Stack

Note that removing the stack removes all services (all task replicas) and networks.

```bash
Removing service mystack_ap_bar
Removing service mystack_ap_foo
Removing network mystack_my_net_foo
Removing network mystack_my_net_bar
Removing network mystack_my_net
```

## Stack lb-apache-tomcat

The stack consists of

- A load-balancer (lb service) load balances among Apache HTTP servers (apache service).
- Apache hosts static files (like images) under `/duke`
- Upon receiving request to  prefix `/duke/app` the Apache acts as a reverse-proxy routing requests to Tomcat servers (tomcat service).
- Tomcat hosts a web-app `duke.war`

This way any request to a JSP reaches tomcat via load-balancer and apache. The JSP may include several static files (such as images, css, js). While browser renders the HTML (generated by JSP) requests to static files are answered by apache. This way tomcat is better utilised to handle dynamic content.

### Stack directory structure

The load balancer is implemented using a generic image `cafeduke/lb:latest`. By generic we mean, it is not specific to this project.

However, the apache and tomcat and specific to this project and we have corresponding `cafeduke-apache` and `cafeduke-tomcat` directories having files to build the images.

- The `cafeduke-apache`  routes requests to dynamic prefix `/duke/app` to `tomcat` service.
- The `cafeduke-apache` uses `tomcat` service name as DNS resolvable hostname.
- The `cafeduke-tomcat` hosts  `duke.war` that includes static files. For example, `pic.jsp` generates path to image using `<c:url value="relative path to static file"/>`

```bash
> tree -P "docker-compose.yml|Dockerfile"
.
├── cafeduke-apache
│   ├── conf
│   ├── Dockerfile
│   └── htdocs
│       └── duke
│           └── images
├── cafeduke-tomcat
│   └── Dockerfile
└── docker-compose.yml

# Note: We are using service name 'tomcat' as DNS hostname
> cat cafeduke-apache/conf/duke.conf
ProxyPass "/duke/app" "http://tomcat:8080/duke"
ProxyPassReverse "/duke/app" "http://tomcat:8080/duke"

# pic.jsp from duke.war
...
<c:url value="/images/${param.name}" var="urlPic" />
<img src="${urlPic}" height="200px"/>
...
```

### Stack compose file

We have 3 services -- lb(2 replicas), apache(3 replicas) and tomcat(3 replicas)

- The images for apache (`cafeduke/apache-stack-demo:latest`) and tomcat (`cafeduke/tomcat-stack-demo:latest`) are built
- Both apache's and tomcat's images are tagged `latest`
- The `build > context` YAML element specifies the directory to find `Dockerfile`
- Service `tomcat` is in network `backend` and service `lb` is in network `front_end`. However, service `apache` is on both networks `back_end` as well as `front_end`. This is because `apache` needs to route dynamic requests to `tomcat`, so they need to have a **network in common**  in order to communicate.
- The `depends_on` element ensures the dependent services are started ensuring proper order of bring up the stack.

> Services need a network in common to communicate with each other.

```bash
version: "3.8"

services:

  tomcat:
    image: cafeduke/tomcat-stack-demo:latest
    build:
      context: cafeduke-tomcat
    ports:
      - "18801:8080"
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    networks:
      back_end:

  apache:
    image: cafeduke/apache-stack-demo:latest
    build:
      context: cafeduke-apache
    ports:
      - "8801:80"
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    networks:
      front_end:
      back_end:
    depends_on:
      - tomcat

  lb:
    image: cafeduke/lb:latest
    ports:
      - "8080:80"
    environment:
      - ORIGIN_SERVER=apache
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    networks:
      front_end:
    depends_on:
      - apache

networks:
  front_end:
  back_end:
```

### Build images of the stack

A docker stack is a feature of swarm that typically manages multiple nodes. A node can go down and corresponding tasks will have to be **automatically** recreated on **other** node(s). Hence, the images used to create these tasks cannot be locally stored on a machine -- It has to be uploaded (pushed) to [docker-hub](https://hub.docker.com).

> A docker stack gives first preference to images uploaded to docker-hub.

We can use `docker-compose` to build all the images specific to the stack in one go. Note that we do not have a `build` element for the generic service `lb`.

```bash
# Build images for the stack
> cd Learn/Docker/04-swarm/swarm-stack-lb-apache-tomcat
> docker-compose build

# No build need for lb
lb uses an image, skipping
Building with native build. Learn about native build in Compose here: https://docs.docker.com/go/compose-native-build/

# Service tomcat. Image cafeduke/tomcat-stack-demo:latest
Building tomcat
Sending build context to Docker daemon  268.8kB

Step 1/3 : FROM tomcat:9
 ---> 040bdb29ab37
Step 2/3 : WORKDIR /usr/local/tomcat/webapps
 ---> Using cache
 ---> 19e71d584577
Step 3/3 : COPY duke.war .
 ---> Using cache
 ---> fdae51fdfe1e
Successfully built fdae51fdfe1e
Successfully tagged cafeduke/tomcat-stack-demo:latest

# Service apache. Image cafeduke/apache-stack-demo:latest
Building apache
Sending build context to Docker daemon  593.9kB

Step 1/4 : FROM hypoport/httpd-cgi:latest
 ---> 1cb56ef98369
Step 2/4 : WORKDIR /usr/local/apache2
 ---> Using cache
 ---> 3962d9b56e8e
Step 3/4 : COPY htdocs/ htdocs/
 ---> Using cache
 ---> 031a9b2abcdc
Step 4/4 : COPY conf/ conf/
 ---> Using cache
 ---> 53aa4cd103a7
Successfully built 53aa4cd103a7
Successfully tagged cafeduke/apache-stack-demo:latest

```
### Deploy using docker stack deploy

A stack looks first at  [docker-hub](https://hub.docker.com) for the image
- A stack deploy shall first look for an image (along with tag) in [docker-hub](https://hub.docker.com).
- If found this shall be used (even if a more recent one exists among local machine's docker images), otherwise the local docker image shall be used warning the user.

We begin by creating the stack using `docker stack deploy` command. We use the same `docker-compose.yml` file, that is used to build images as well.

- The stack creates the necessary networks and tasks
- The networks and services are appended with the stack name (In this case `mystack`)

> A single YAML file is used to build images (development) as well as deploy to a stack (production).

Push images to  [docker-hub](https://hub.docker.com)

- Login using `docker login` command
- Push images to  [docker-hub](https://hub.docker.com) using `docker push <image>` command

```bash
# Deploy
> docker stack deploy --compose-file docker-compose.yml mystack
Ignoring unsupported options: build
Creating network mystack_front_end
Creating network mystack_back_end
Creating service mystack_apache
Creating service mystack_lb
Creating service mystack_tomcat
​```bash

### Push images to hub.docker.com
​```bash
# Login using hub.docker.com's login/password
> docker login

# Push images to hub.docker.com
> docker push cafeduke/apache-stack-demo:latest
The push refers to repository [docker.io/cafeduke/apache-stack-demo]
latest: digest: sha256:67cc9a2d7448b4879c9e12cd7b0d9fc5a4554d46a4e51b993376562ade44fdcb size: 2820

> docker push cafeduke/tomcat-stack-demo:latest
The push refers to repository [docker.io/cafeduke/tomcat-stack-demo]
latest: digest: sha256:3b5a80d3ccd8bd385935299071661ca9ef1db397d7ef248d40ca8dbdf2449691 size: 2631
```
### Update stack using docker stack deploy
The `docker stack deploy` command is used to update the stack as well. The new stack could have new services, scaled up/down replicas of a given service, updated config or even a new tag (version) of the service image. All services shall be updated accordingly.

> The command to create a stack is used to update the stack as well

```bash
# The command to create a stack is used to update the stack as well
> docker stack deploy --compose-file docker-compose.yml mystack
Ignoring unsupported options: build
Updating service mystack_tomcat (id: s6xdsxghfvbfc8hdlufwf7531)
Updating service mystack_apache (id: lqb65l7h758n7m22mmvtc12qh)
Updating service mystack_lb (id: hwp0r0dqi35v0ylzb00ebqxhh)
```

### Access pages and verify working

- Verify working of http://localhost:8080/duke/app/pic.jsp?name=duke.png
- Verify working of http://localhost:8080/duke/app/pic.jsp?name=penguin.png
- Note that the `ServerIP` changes among the IPs of the `tomcat` server indicating load balancing.

### Verify service is running with the expected **latest** version

Consider this
- We have an existing stack up and running.
- We now have made changes some of our docker images, built them, tagged them as **latest** and pushed to  [docker-hub](https://hub.docker.com)
- Now when we execute `docker stack deploy` the image of the running service is `<image>:latest`, yet it does not have the changes in the [docker-hub](https://hub.docker.com)  also tagged `<image>:latest`. What will docker do?
  - Thanks to `--resolve-image` option of `docker stack deploy` which is by default `always` , docker shall check if the `hash (SHA 256)` of the image at [docker-hub](https://hub.docker.com) matches the one used by the running service. If they are not the same, the service shall be updated with the  [docker-hub](https://hub.docker.com) image.

```bash
# Check the hash at docker-hub
# -----------------------------------------------------------------------------
# https://hub.docker.com/r/cafeduke/apache-stack-demo/tags)
Note the DIGEST (hash): 67cc9a2d7448b48

# Check the hash of the image built locally
# -----------------------------------------------------------------------------
> docker image ls --digests cafeduke/apache-stack-demo | grep latest
cafeduke/apache-stack-demo   latest    sha256:67cc9a2d7448b4879c9e12cd7b0d9fc5a4554d46a4e51b993376562ade44fdcb   53aa4cd103a7   22 minutes ago   112MB

# Check the hash of the running service 'apache'
# -----------------------------------------------------------------------------
> docker service inspect mystack_apache | grep -i sha | head -1
"Image": "cafeduke/apache-stack-demo:latest@sha256:67cc9a2d7448b4879c9e12cd7b0d9fc5a4554d46a4e51b993376562ade44fdcb",

```

# Swarm multi node

Multiple nodes (machines VM/physical) can be simulated using `docker-machine` and Oracle VirtualBox softwares.

## Installation

### Docker machine

- Install [docker-machine](https://docs.docker.com/machine/install-machine)
- Verify installation

```bash
> docker-machine --version
docker-machine version 0.16.0, build 702c267f
```

### Install Oracle VirtualBox

- Install [Oracle VirtualBox](https://www.virtualbox.org/wiki/Linux_Downloads)

## Create and manage nodes

> Each node is like a separate physical machine.

```bash
# Create node
> docker-machine create node1 nod2 node3

# Verify working of node
> docker-machine ssh node1
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

# Stop nodes
> docker-machine stop node1 node2 node3

# Start nodes
> docker-machine start node1 node2 node3
docker-machine start node1 node2 node3
Starting "node3"...
Starting "node2"...
Starting "node1"...
...
...
Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.

# List nodes -- Note the IPs of the node
> docker-machine ls
NAME    ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
node1   -        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.12
node2   -        virtualbox   Running   tcp://192.168.99.101:2376           v19.03.12
node3   -        virtualbox   Running   tcp://192.168.99.102:2376           v19.03.12
```

> docker-machine ls command lists all nodes along with their IP.

## Update docker client to point to node

Docker client `docker` is just command (binary) that points to a given node based on environment variables.

```bash
# Run the command to get the list of environent vairalbes to set OR simply run "eval $(docker-machine env node1)"
> docker-machine env node1
export DOCKER_TLS_VERIFY="1"
export DOCKER_HOST="tcp://192.168.99.100:2376"
export DOCKER_CERT_PATH="/home/raghu/.docker/machine/machines/node1"
export DOCKER_MACHINE_NAME="node1"
# Run this command to configure your shell:
# eval $(docker-machine env node1)

# Switching to node1
raghu-pc ~/work> eval $(docker-machine env node1)
raghu-pc ~/work> bash
Sourcing /home/raghu/.bashrc
Sourcing /home/raghu/.profile

# Note that 'node1' is the new host
node1 ~/work> docker info | grep Name
 Name: node1
```

## Manage nodes

A node is like a new physical machine and is not part of any swarm. A node can join a swarm as a **master** or **worker**.

### Make node1 the lonely Leader

```bash
# Switching to node1
raghu-pc ~/work> eval $(docker-machine env node1)

# Initalize swarm on the node1 and make it the 'Leader'
node1 ~/work> docker swarm init --advertise-addr 192.168.99.100
Swarm initialized: current node (qy4muu46yuadq5mgyrojzsp2o) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-5k21hnpwpzuvfjbh44nhnnaol 192.168.99.100:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

# Command to add a node as manager
> docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-8qr70bbax49dtienbi9f0e0ao 192.168.99.100:2377


# Currently there is only node1 and it's the 'Leader'
> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o *   node1      Ready     Active         Leader           19.03.12

```
A node can join as either mananger or worker by executing corresponding command on the node.
- Join as worker: Use command `docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-5k21hnpwpzuvfjbh44nhnnaol 192.168.99.100:2377`
- Join as manager: Use command `docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-8qr70bbax49dtienbi9f0e0ao 192.168.99.100:2377`

> Docker swarm initialisation using `docker swarm init` gives commands to make nodes join as worker or manager

### Make node2 join the swarm as worker

```bash
raghu-pc ~/work> eval $(docker-machine env node2)

node2 ~/work> docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-5k21hnpwpzuvfjbh44nhnnaol 192.168.99.100:2377
This node joined a swarm as a worker.

```

#### Verify that worker cannot view/modify swarm cluster

```bash
node2 ~/work> docker node ls
Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.
```

#### Verify that we now have two nodes in the swarm

```bash
node1 ~/work> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o *   node1      Ready     Active         Leader           19.03.12
ybw0jqm936fwhwz1gvmnfcgv5     node2      Ready     Active                          19.03.12
```

### Promote node2

- The manager status `Reachable` indicates that the node is a manager and is reachable.

```bash
node1 ~/work> docker node promote node2
Node node2 promoted to a manager in the swarm.
node1 ~/work> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o *   node1      Ready     Active         Leader           19.03.12
ybw0jqm936fwhwz1gvmnfcgv5     node2      Ready     Active         Reachable        19.03.12
```

### Make node3 join the swarm directly as manager

```bash
node3 ~/work> docker swarm join --token SWMTKN-1-1ygd8r5s56qzcph7cm9m0ii0eyp0hiio1gxmiomi5ofkei2l5c-8qr70bbax49dtienbi9f0e0ao 192.168.99.100:2377
This node joined a swarm as a manager.
```

### Verify node1 is no more lonely

```bash
node1 ~/work> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o *   node1      Ready     Active         Leader           19.03.12
ybw0jqm936fwhwz1gvmnfcgv5     node2      Ready     Active         Reachable        19.03.12
tr6kf9ec72b3wi4bdn1v39168     node3      Ready     Active         Reachable        19.03.12
```

### Demote node1

```bash
# Demote node1
node1 ~/work> docker node demote node1
Manager node1 demoted in the swarm.

# Note that node2 is now the Leader
node2 ~/work> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o     node1      Ready     Active                          19.03.12
ybw0jqm936fwhwz1gvmnfcgv5 *   node2      Ready     Active         Leader           19.03.12
tr6kf9ec72b3wi4bdn1v39168     node3      Ready     Active         Reachable        19.03.12

```

### Restore status

```bash
# We are promoting node1 back to manager
node2 ~/work> docker node promote node1
Node node1 promoted to a manager in the swarm.

# However, note that node2 remains the 'Leader'
node2 ~/work> docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
qy4muu46yuadq5mgyrojzsp2o     node1      Ready     Active         Reachable        19.03.12
ybw0jqm936fwhwz1gvmnfcgv5 *   node2      Ready     Active         Leader           19.03.12
tr6kf9ec72b3wi4bdn1v39168     node3      Ready     Active         Reachable        19.03.12

```

# Swarm stack on multi-node



## Stack lb-apache-tomcat

```bash
> docker stack deploy --compose-file docker-compose.yml mystack
Creating network mystack_front_end
Creating network mystack_back_end
Creating service mystack_lb
Creating service mystack_tomcat
Creating service mystack_apache

> docker stack ls
NAME      SERVICES   ORCHESTRATOR
mystack   3          Swarm

> docker stack services mystack
ID             NAME             MODE         REPLICAS   IMAGE                               PORTS
hd9ggc261a2l   mystack_apache   replicated   3/3        cafeduke/apache-stack-demo:latest   *:8801->80/tcp
59fsvfei7jpm   mystack_lb       replicated   2/2        cafeduke/lb:latest                  *:8080->80/tcp
emlju8x06l9k   mystack_tomcat   replicated   3/3        cafeduke/tomcat-stack-demo:latest   *:18801->8080/tcp

> docker service ps mystack_apache mystack_lb mystack_tomcat --filter desired-state=running
ID             NAME               IMAGE                               NODE      DESIRED STATE   CURRENT STATE           ERROR     PORTS
hsh5ndry3d20   mystack_apache.1   cafeduke/apache-stack-demo:latest   node1     Running         Running 6 minutes ago
j2txqvfi2jrg   mystack_apache.2   cafeduke/apache-stack-demo:latest   node3     Running         Running 7 minutes ago
ynzr9m2s98i4   mystack_apache.3   cafeduke/apache-stack-demo:latest   node2     Running         Running 7 minutes ago
jms3xqe1mvh3   mystack_lb.1       cafeduke/lb:latest                  node1     Running         Running 6 minutes ago
w207xkrcd40x   mystack_lb.2       cafeduke/lb:latest                  node3     Running         Running 7 minutes ago
hpi42q3p3ujo   mystack_tomcat.1   cafeduke/tomcat-stack-demo:latest   node2     Running         Running 6 minutes ago
9pbxttf4chdc   mystack_tomcat.2   cafeduke/tomcat-stack-demo:latest   node3     Running         Running 5 minutes ago
x86x8qze8w2y   mystack_tomcat.3   cafeduke/tomcat-stack-demo:latest   node1     Running         Running 5 minutes ago

```

### Access pages and verify working

- Get the IPs of nodes using `docker-machine ls`
- Verify working of http://192.168.99.100:8080/duke/app/pic.jsp?name=duke.png
- Verify working of http://192.168.99.100:8080/duke/app/pic.jsp?name=penguin.png
- Note that the `ServerIP` changes among the IPs of the `tomcat` server indicating load balancing.

**Note tomcat IP from each node**

```bash
node1 ~/work> docker network inspect mystack_back_end | grep -A 4 "tomcat" | egrep "Name|IPv4"
"Name": "mystack_tomcat.3.x86x8qze8w2y1a4k97o04vt16",
"IPv4Address": "10.0.2.5/24",

node2 ~/work> docker network inspect mystack_back_end | grep -A 4 "tomcat" | egrep "Name|IPv4"
"Name": "mystack_tomcat.1.hpi42q3p3ujocefjo6atcuo8n",
"IPv4Address": "10.0.2.3/24",

node3 ~/work> docker network inspect mystack_back_end | grep -A 4 "tomcat" | egrep "Name|IPv4"
"Name": "mystack_tomcat.2.9pbxttf4chdch42vcr4k5issd",
"IPv4Address": "10.0.2.4/24",
```

**Send simultaneous requests and verify load-balancing**

```bash
> jget -u "http://192.168.99.100:8080/duke/app/Snoop.jsp" -mode MSC -n 10 -o file; grep ServerIP file*
file0001.html:ServerIP=10.0.2.3
file0002.html:ServerIP=10.0.2.4
file0003.html:ServerIP=10.0.2.3
file0004.html:ServerIP=10.0.2.5
file0005.html:ServerIP=10.0.2.3
file0006.html:ServerIP=10.0.2.5
file0007.html:ServerIP=10.0.2.4
file0008.html:ServerIP=10.0.2.4
file0009.html:ServerIP=10.0.2.4
file0010.html:ServerIP=10.0.2.3
```

### Verify routing mesh

We see that requesting any of the nodes

```bash
raghu-pc ~/work> curl -s -I "http://192.168.99.100:8080/duke/app/hello.jsp" | head -1
HTTP/1.1 200

raghu-pc ~/work> curl -s -I "http://192.168.99.101:8080/duke/app/hello.jsp" | head -1
HTTP/1.1 200

raghu-pc ~/work> curl -s -I "http://192.168.99.102:8080/duke/app/hello.jsp" | head -1
HTTP/1.1 200
```























