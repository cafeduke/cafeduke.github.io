<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Meta data -->
    <meta charset="utf-8">
<meta name="viewport" content="initial-scale=1, shrink-to-fit=no, width=device-width">
<meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Window title -->
    <title>Vanishing and exploding gradients problem</title>

    <!-- CSS -->
    <!-- Material fonts from 'https://fonts.google.com' -->
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i,700,700i" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,300i,400,400i,500,500i,700,700i" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,500,500i,700,700i" rel="stylesheet"> 
<link href="https://fonts.googleapis.com/css?family=Vollkorn:300,300i,400,400i,500,500i,700,700i" rel="stylesheet"> 

<!-- Material Icon -->
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

<!-- Font Awesome Icon -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

<!-- Add Material CSS. Replace Bootstrap CSS -->
<link href="/assets/daemonite-material/css/material.min.css" rel="stylesheet">

<!-- Material Color Palette - CSS Classes -->
<link href="/assets/material-design-color-palette/css/material-design-color-palette.css" rel="stylesheet">

<!-- CafeDuke CSS - Project specific Classes  -->
<link href="/assets/css/main.css" rel="stylesheet" type="text/css">

<!-- CafeDuke Syntax CSS - Code syntax highlight -->
<link href="/assets/css/duke-dark.css" rel="stylesheet" type="text/css">


<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <!-- Navigation bar -->
    <nav class="navbar navbar-expand-lg navbar-dark mdc-bg-purple-900">
    
    

    <!-- Collapse Icon -->
    <a id="sidebar-toggle" class="navbar-brand text-white" href="">
        <i class="material-icons md-24">menu</i>
    </a>

    <!-- NavBar Heading -->
    <a class="navbar-brand text-white" href="/">Cafe Duke Notes</a>

    <!-- Collapse button -->
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">

        <!-- Links -->
        <ul class="navbar-nav mr-auto">


        </ul>
        <!-- Links -->

        <!-- Search form
        <form class="form-inline">
            <input class="form-control mr-sm-2" type="text" placeholder="Search" aria-label="Search">
        </form>
         -->

    </div>
    <!-- Collapsible content -->

</nav>


    <!-- Row: Content -->
    <div class="container-fluid m-0 p-0 h-100">

        <div class="row h-100 no-gutters">

            <!-- Column: Sidebar -->
            <div id="sidebar">

    <ul class="nav flex-column">

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/">
                <i class="fas fa-home fa-lg"></i>
                <span class="nav-link-text">Home</span>
            </a>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/java">
                <i class="fab fa-java fa-2x"></i>
                <span class="nav-link-text">Java</span>
            </a>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/git">
                <i class="fab fa-git fa-lg"></i>
                <span class="nav-link-text">Git</span>
            </a>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/cloud">
                <i class="material-icons md-24">cloud</i>
                <span class="nav-link-text">Cloud</span>
            </a>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" data-toggle="collapse" data-target="#child-dl" href="#">
                <i class="fas fa-cogs fa-lg"></i>
                <span class="nav-link-text">Deep Learning</span>
            </a>
            <div id="child-dl" class="collapse">
                <ul class="nav flex-column">
                    <li class="nav-item">
                        <a class="nav-link d-flex align-items-center" href="/deeplearning">
                            <span class="nav-link-text">Core</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link d-flex align-items-center" href="/deeplearning/strategy">
                            <span class="nav-link-text">Strategy</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link d-flex align-items-center" href="/deeplearning/cnn">
                            <span class="nav-link-text">CNN</span>
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link d-flex align-items-center" href="/deeplearning/rnn">
                            <span class="nav-link-text">RNN</span>
                        </a>
                    </li>
                </ul>
            </div>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/django">
                <i class="fa fa-lg">dj</i>
                <span class="nav-link-text">Django</span>
            </a>
        </li>

        <li class="nav-item">
            <a class="nav-link d-flex align-items-center" href="/spring">
                <i class="fas fa-leaf fa-lg"></i>
                <span class="nav-link-text">Spring</span>
            </a>
        </li>

    </ul>
</div>


            <!-- Column: Content Wrapper -->
            <div class="col">
                <div id="content" class="w-100">
<div id="post-wrap" class="row d-flex justify-content-center">

    <div class="card col-md-10 col-xs-12 mt-5">

        <div class="card-header">
            <h1 class="post-title">Vanishing and exploding gradients problem</h1>
            <h6 class="card-subtitle post-meta"> 
                Monday, 27 November 2017
                
            </h6>            
        </div>

        <hr class="post-ruler">

        <div class="card-body">
            <nav>
  <h4>Table of Contents</h4>
<ol id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li>
<a href="#basic-components-of-nn" id="markdown-toc-basic-components-of-nn">Basic components of NN</a>    <ol>
      <li><a href="#gradient-descent" id="markdown-toc-gradient-descent">Gradient descent</a></li>
      <li><a href="#activation-functions" id="markdown-toc-activation-functions">Activation Functions</a></li>
    </ol>
  </li>
  <li>
<a href="#analyzing-nn-with-4-layers" id="markdown-toc-analyzing-nn-with-4-layers">Analyzing NN with 4 layers</a>    <ol>
      <li>
<a href="#forward-propagation" id="markdown-toc-forward-propagation">Forward propagation</a>        <ol>
          <li><a href="#generic-equation" id="markdown-toc-generic-equation">Generic Equation</a></li>
          <li><a href="#equations-for-the-layers" id="markdown-toc-equations-for-the-layers">Equations for the layers</a></li>
        </ol>
      </li>
      <li>
<a href="#back-propagation" id="markdown-toc-back-propagation">Back propagation</a>        <ol>
          <li><a href="#generic-equation-1" id="markdown-toc-generic-equation-1">Generic Equation</a></li>
          <li><a href="#equations-for-the-layers-1" id="markdown-toc-equations-for-the-layers-1">Equations for the layers</a></li>
          <li><a href="#solving-for-dw_1" id="markdown-toc-solving-for-dw_1">Solving for $dW_{1}$</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
<a href="#vanishing-gradient" id="markdown-toc-vanishing-gradient">Vanishing gradient</a>    <ol>
      <li><a href="#why-sigmoid-activation-may-cause-vanishing-gradient" id="markdown-toc-why-sigmoid-activation-may-cause-vanishing-gradient">Why sigmoid activation may cause vanishing gradient?</a></li>
      <li><a href="#how-relu-activation-prevents-vanishing-gradient" id="markdown-toc-how-relu-activation-prevents-vanishing-gradient">How relu activation prevents vanishing gradient?</a></li>
    </ol>
  </li>
  <li>
<a href="#exploding-gradient" id="markdown-toc-exploding-gradient">Exploding gradient</a>    <ol>
      <li><a href="#how-higher-weights-results-in-exploding-gradient" id="markdown-toc-how-higher-weights-results-in-exploding-gradient">How higher weights results in exploding gradient?</a></li>
      <li><a href="#initializing-weights-to-mitigate-gradient-explosion" id="markdown-toc-initializing-weights-to-mitigate-gradient-explosion">Initializing weights to mitigate gradient explosion</a></li>
    </ol>
  </li>
</ol>

</nav>

<h1 id="introduction">Introduction</h1>

<p>In NN, especially deep neural networks the derivative can get exponentially small or exponentially large referred to as vanishing or exploding gradient respectively.</p>

<h1 id="basic-components-of-nn">Basic components of NN</h1>

<h2 id="gradient-descent">Gradient descent</h2>

<p>Gradient descent involves iterating over</p>

<ul>
  <li>Finding the gradient $$dW = \frac{\partial J}{\partial W}$$ of the weights $$w$$ and bias $$b$$ w.r.t the cost function $$J(w,b)$$ so as to minimize the cost function.</li>
  <li>Update the weights with the gradient</li>
</ul>

<p>$$
\begin{aligned}
&amp; foreach \ W \ across \ layers \ \ { <br>
&amp; \ \ \ \ \ \ W = W - \alpha \frac{\partial}{\partial W} (J) <br>
&amp; }
\end{aligned}
$$</p>

<p>If the gradient (derivatives matrix $$dW$$) becomes small, gradient descent and hence the learning shall become very slow referred to as <strong>vanishing gradient</strong>.</p>

<h2 id="activation-functions">Activation Functions</h2>

<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>g(z)</th>
      <th>Range of g(z)</th>
      <th>g’(z)</th>
      <th>Values for g’(z)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sigmoid</td>
      <td>\[\frac{1}{1+e^{-z}}\]
</td>
      <td>\[0 \ to \ 1\]
</td>
      <td>g(z) * (1-g(z))</td>
      <td>\[0 \ to \ 0.25\]
</td>
    </tr>
    <tr>
      <td>relu</td>
      <td>\[\begin{cases} 0 &amp;amp; z &lt; 0 \ z &amp;amp; z &gt;= 0  \end{cases}\]
</td>
      <td>\[0 \ to \ z\]
</td>
      <td>\[0 \ or \ 1\]
</td>
      <td>\[0 \ or \ 1\]
</td>
    </tr>
  </tbody>
</table>

<p><strong>Note:</strong></p>

<ul>
  <li>The derivative of sigmoid activation function $$g’(z)$$ <strong>ranges</strong> but has a maximum value of $$0.25$$</li>
  <li>The derivative of relu function is <strong>binary</strong>  $$\Rightarrow$$  either 0 or 1</li>
</ul>

<h1 id="analyzing-nn-with-4-layers">Analyzing NN with 4 layers</h1>

<p>Lets take an example of neural network that has 4 layers and see how the equations for back prop expand.</p>

<p><img src="/assets/images/4LayerNN.jpg" alt="4LayerNN"></p>

<h2 id="forward-propagation">Forward propagation</h2>

<h3 id="generic-equation">Generic Equation</h3>

<p>The generic equation for forward propagation is as follows. Here, g(z) is an activation function such as sigmoid, tanh,  relu or softmax function
$$
\begin{aligned}
Z_{l} &amp;= W_{l}*A_{l-1} + b_{l} <br>
A_{l} &amp;= g(Z_{l})
\end{aligned}
$$</p>

<h3 id="equations-for-the-layers">Equations for the layers</h3>

<p>The equation can be extended to various layers as follows:</p>

<table>
  <thead>
    <tr>
      <th>Layer 1</th>
      <th>Layer 2</th>
      <th>Layer 3</th>
      <th>Layer4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\[Z1 = W1*X + b1\]
</td>
      <td>\[Z2 = W2*A1 + b2\]
</td>
      <td>\[Z3 = W3 * A2 + b3\]
</td>
      <td>\[Z4 = W4*A3 + b4\]
</td>
    </tr>
    <tr>
      <td>\[A1=g(Z1)\]
</td>
      <td>\[A2=g(Z2)\]
</td>
      <td>\[A3=g(Z3)\]
</td>
      <td>\[A4=g(Z4)\]
</td>
    </tr>
  </tbody>
</table>

<h2 id="back-propagation">Back propagation</h2>
<h3 id="generic-equation-1">Generic Equation</h3>

<p>The equations of the back propagation can be derived by differentiating (and then generalizing) the equations tabulated above.
$$
\begin{aligned}
dW_{l} &amp;= dZ_{l} * A_{l-1} <br>
dZ_{l} &amp;= dA_{l} * g’(Z_{l}) = dZ_{l+1} * W_{l+1} * g’(Z_{l}) <br>
db_{l} &amp;= dZ_{l}
\end{aligned}
$$</p>

<h3 id="equations-for-the-layers-1">Equations for the layers</h3>

<table>
  <thead>
    <tr>
      <th>Layer 1</th>
      <th>Layer 2</th>
      <th>Layer 3</th>
      <th>Layer4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\[dW_{1} = dZ_{1}*X\]
</td>
      <td>\[dW_{2} = dZ_{2}*A_{1}\]
</td>
      <td>\[dW_{3} = dZ_{3}*A_{2}\]
</td>
      <td>\[dW_{4} = dZ_{4}*A_{3}\]
</td>
    </tr>
    <tr>
      <td>\[dZ_{1} = dZ_{2} * W_{2} * g’(Z_{1})\]
</td>
      <td>\[dZ_{2} = dZ_{3} * W_{3} * g’(Z_{2})\]
</td>
      <td>\[dZ_{3} = dZ_{4} * W_{4} * g’(Z_{3})\]
</td>
      <td>\[dZ_{4} = dA_{4} * g’(Z_{4})\]
</td>
    </tr>
  </tbody>
</table>

<h3 id="solving-for-dw_1">Solving for $dW_{1}$</h3>

<p>$$
\begin{aligned}
dW_{1} &amp;= dZ_{1} * X <br>
       &amp;= dZ_{2} * X * W_{2} * g’(Z_{1}) <br>
       &amp;= dZ_{3} * X * W_{2} * W_{3} * g’(Z_{1}) * g’(Z_{2})  <br>
       &amp;= dZ_{4} * X * W_{2} * W_{3} *  W_{4} * g’(Z_{1}) * g’(Z_{2}) * g’(Z_{3})  <br>
\end{aligned}
$$</p>

<h1 id="vanishing-gradient">Vanishing gradient</h1>

<h2 id="why-sigmoid-activation-may-cause-vanishing-gradient">Why sigmoid activation may cause vanishing gradient?</h2>

<p>Expanding forward propagation for 
$$
\begin{aligned}
dW_{1} &amp;= dZ_{1} * X <br>
       &amp;= dZ_{2} * X * W_{2} * g’(Z_{1}) <br>
       &amp;= dZ_{3} * X * W_{2} * W_{3} * g’(Z_{1}) * g’(Z_{2})  <br>
       &amp;= dZ_{4} * X * W_{2} * W_{3} *  W_{4} * g’(Z_{1}) * g’(Z_{2}) * g’(Z_{3})  <br>
\end{aligned}
$$
Expanding the equation for the gradient  $$dW1$$ for a 4 layered neural network gives us the following equation.</p>

<p>$$
dW_{1}= dZ_{4} * X * W_{2} * W_{3} *  W_{4} * g’(Z_{1}) * g’(Z_{2}) * g’(Z_{3})
$$</p>

<p><strong>Note</strong></p>

<ul>
  <li>Weights $$W$$ generally have a value less than 1</li>
  <li>If sigmoid activation function is used g’(Z) will have a max value of 0.25 Essentially, both W and g’(Z) shall be values less than 1.</li>
</ul>

<p>$$
dW \tilde= (x &lt; 1)^{2(l-1)}
$$</p>

<ul>
  <li>In a deep neural network, the value of $$l$$ may be reach 50 or more which will make the derivative insignificant resulting in vanishing gradient.</li>
</ul>

<h2 id="how-relu-activation-prevents-vanishing-gradient">How relu activation prevents vanishing gradient?</h2>

<p>The derivative is g’(z) in case of relu is either 0 or 1. When the derivative is 1 (which is mostly the case since z is rarely negative), The equation for 4 layer neural network boils down to the following</p>

<p>$$
dW_{1}= dZ_{4} * X * W_{2} * W_{3} *  W_{4} * 1 * 1 * 1
$$
This reduces the effect of raising a value less than 1 to ha high power. Thus working well even for deeper networks.</p>

<h1 id="exploding-gradient">Exploding gradient</h1>

<h2 id="how-higher-weights-results-in-exploding-gradient">How higher weights results in exploding gradient?</h2>

<p><strong>Forward Propagation</strong>: Expanding the equation for the forward propagation  $$Z_{4}$$ for a 4 layered neural network gives us the following equation.</p>

<p>For simplification, consider the following</p>

<ul>
  <li>Ignore bias term $$b$$</li>
  <li>g(z)  = z. Hence $$A_{l} = g(Z_{l}) = Z_{l}$$</li>
</ul>

<p>$$
\begin{aligned}
Z_{4} &amp;= A_{3} * W_{4} <br>
      &amp;= Z_{3} * W_{4} <br>
      &amp;= A_{2} * W_{4} * W_{3} <br>
      &amp;= Z_{2} * W_{4} * W_{3} <br>
      &amp;= A_{1} * W_{4} * W_{3} * W_{2} <br>
      &amp;= Z_{1} * W_{4} * W_{3} * W_{2} \</p>

<p>\end{aligned}
$$</p>

<p><strong>Back Propagation</strong>: Expanding the equation for the gradient  $$dW1$$ for a 4 layered neural network gives us the following equation.
$$
dW_{1}= dZ_{4} * X * W_{2} * W_{3} *  W_{4} * g’(Z_{1}) * g’(Z_{2}) * g’(Z_{3})
$$
<strong>Note</strong>:</p>

<ul>
  <li>Even if W is slightly greater than 1, the value of Z shall increase exponentially with deeper NN.</li>
  <li>With greater Z the value of dZ  shall increase</li>
  <li>From back propagation equation we find the value of dW shall increase resulting in <strong>exploding gradient</strong>.</li>
</ul>

<h2 id="initializing-weights-to-mitigate-gradient-explosion">Initializing weights to mitigate gradient explosion</h2>

<p>The following weight initialization has worked well in a neural network for a given layer $$l$$. Different weight initialization methods have worked well for different activation functions as given below.</p>

<table>
  <thead>
    <tr>
      <th>Activation function</th>
      <th>Weight initialization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Relu</td>
      <td>\[W_{l} = random() * \sqrt{\frac{2}{n_{l-1}}}\]
</td>
    </tr>
    <tr>
      <td>tanh</td>
      <td>\[W_{l} = random() * \sqrt{\frac{1}{n_{l-1}}}\]
</td>
    </tr>
  </tbody>
</table>

<p>Here,</p>

<ul>
  <li>
    <p>$$W_{l}$$ is the weights for the $$l^{th}$$ layer</p>
  </li>
  <li>
    <p>$$n_{l-1}$$ gives the number of neurons in layer  $$l$$</p>
  </li>
  <li>
    <p>random() returns a random float between 0 and 1</p>
  </li>
</ul>

<blockquote>

  <p>Initialization for tanh is called Xavier initialization</p>

</blockquote>

        </div>

        <div class="card-footer">
            
        </div>

    </div>

</div>
</div>
                <!-- <div id="footer" class="w-100"><div class="footer">
    <div class="footer-col footer-col-1">
    <ul class="contact-list">
      <li>
        <span class="site-author">
           
             Raghunandan.Seshadri
           
        </span>
      </li>

      
      <li><a href="mailto:raghubs81@gmail.com">raghubs81@gmail.com</a></li>
      

      
      <li>
        <a href="https://github.com/cafeduke"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">cafeduke</span></a>

      </li>
      

    </ul>
    </div>

    <div class="footer-col footer-col-2">
     <ul class="contact-list">
        
           <li>Duke notes. Happy learning!</li>
        
     </ul>
    </div>
</div></div> -->
            </div>
        </div>
    </div>

    <!-- JS -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"></script>

<!-- Popper -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>

<!-- Bootstrap -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>

<!-- Material JavaScript on top of Bootstrap JavaScript -->
<script src="/assets/daemonite-material/js/material.min.js"></script>

<!-- Include and configure MathJax -->
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
      jax: ["input/AsciiMath", "input/TeX", "input/MathML", "output/HTML-CSS"],
      asciimath2jax: {
         delimiters: [['`','`'], ['$$','$$'], ['$','$']]
      },
      "HTML-CSS": {
         preferredFont:"TeX", availableFonts:["STIX","TeX"]
      }
   });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js"></script>

<script src="/assets/js/mermaid.min.js"></script>

<!-- Google analytics -->


<!-- Sidebar -->
<script src="/assets/js/sidebar.js"></script>



   </body>

</html>
